{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import checklist\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.editor import Editor\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "import numpy as np\n",
    "from pattern.en import sentiment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Where', 'in', 'the', 'world', 'is', 'Iguazu', '?'], ['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'])\n",
      "12543\n",
      "['Davis', 'spokesman', 'Steve', 'Maviglio', 'said', 'the', 'governor', 'felt', '\"', 'betrayed', '\"', 'by', 'the', 'actions', 'of', 'Winter', '.']\n",
      "['B-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "def read_iob2_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "train_data= read_iob2_file('./en_ewt-ud-train.iob2')\n",
    "dev_data = read_iob2_file('./en_ewt-ud-dev.iob2')\n",
    "\n",
    "print(train_data[0])\n",
    "print(len(train_data))\n",
    "for sentences in train_data:\n",
    "    if 'Steve' in sentences[0] and 'Maviglio' in sentences[0]:\n",
    "        print(sentences[0])\n",
    "        print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "PAD = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, pad_unk):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "\n",
    "max_len = max([len(x[0]) for x in train_data ])\n",
    "\n",
    "# Create vocabularies for both the tokens\n",
    "# and the tags\n",
    "token_vocab = Vocab(PAD)\n",
    "label_vocab = Vocab(PAD)\n",
    "id_to_token = [PAD]\n",
    "\n",
    "for tokens, tags in train_data:\n",
    "    for token in tokens:\n",
    "        token_vocab.getIdx(token, True)\n",
    "    for tag in tags:\n",
    "        label_vocab.getIdx(tag, True)\n",
    "\n",
    "NWORDS = len(token_vocab.idx2word)\n",
    "NTAGS = len(label_vocab.idx2word)\n",
    "\n",
    "# convert text data with labels to indices\n",
    "def data2feats(inputData, word_vocab, label_vocab):\n",
    "    feats = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    labels = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    for sentPos, sent in enumerate(inputData):\n",
    "        for wordPos, word in enumerate(sent[0][:max_len]):\n",
    "            wordIdx = word_vocab.getIdx(word)\n",
    "            feats[sentPos][wordPos] = wordIdx\n",
    "        for labelPos, label in enumerate(sent[1][:max_len]):\n",
    "            labelIdx = label_vocab.getIdx(label)\n",
    "            labels[sentPos][labelPos] = labelIdx\n",
    "    return feats, labels\n",
    "\n",
    "train_features, train_labels = data2feats(train_data, token_vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changed it to only first 5 data points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to batches\n",
    "num_batches = int(len(train_features)/BATCH_SIZE)\n",
    "train_feats_batches = train_features[:BATCH_SIZE*num_batches].view(num_batches, BATCH_SIZE, max_len)\n",
    "train_labels_batches = train_labels[:BATCH_SIZE*num_batches].view(num_batches, BATCH_SIZE, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model overview: \n",
      "LangID(\n",
      "  (word_embeddings): Embedding(19674, 100)\n",
      "  (bilstm): LSTM(100, 50, batch_first=True)\n",
      "  (hidden_to_tag): Linear(in_features=50, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our model consisting of word embeddings, a single bilstm layer, and an output labels\n",
    "class LangID(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_dim, vocab_dim):\n",
    "        super(LangID, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_dim, embed_dim)\n",
    "        self.bilstm = nn.LSTM(embed_dim, lstm_dim, bidirectional=False, batch_first=True)\n",
    "        self.hidden_to_tag = nn.Linear(lstm_dim, NTAGS)\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # First encode the input into word representations and run the bilstm\n",
    "        word_vectors = self.word_embeddings(inputs)\n",
    "        bilstm_out, _ = self.bilstm(word_vectors)\n",
    "        #  Now combine (concatenate) the last state of each layer\n",
    "        # backward_out = bilstm_out[:,0,-self.lstm_dim:]\n",
    "        # forward_out = bilstm_out[:,-1,:self.lstm_dim]\n",
    "        # bilstm_out = torch.cat((forward_out, backward_out), 1)\n",
    "        # And get the prediction\n",
    "        y = self.hidden_to_tag(bilstm_out)\n",
    "        # return y # softmax this in order to get probs, check out for axis, has to sum up to 1\n",
    "        # print(torch.sum(m(y), dim=1))\n",
    "        return y # Return logits before softmax for predictions\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        # Disable updating the weights\n",
    "        with torch.no_grad():\n",
    "            data_feats, data_labels = data2feats(inputs, token_vocab, label_vocab)\n",
    "\n",
    "            logits = self.forward(data_feats)\n",
    "            probabilities = self.softmax(logits)\n",
    "            # return torch.argmax(probabilities, axis=1)\n",
    "            return torch.argmax(probabilities, 2), probabilities  # Return both probs and labels\n",
    "\n",
    "\n",
    "# define the model\n",
    "langid_model = LangID(DIM_EMBEDDING, LSTM_HIDDEN, NWORDS)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "optimizer = optim.Adam(langid_model.parameters(), lr=LEARNING_RATE)\n",
    "print('model overview: ')\n",
    "print(langid_model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   loss      Train acc.\n",
      "0       279.29    0.9419\n",
      "1       117.44    0.9669\n",
      "2       61.18     0.9820\n",
      "3       36.83     0.9899\n",
      "4       24.54     0.9933\n"
     ]
    }
   ],
   "source": [
    "print('epoch   loss      Train acc.')\n",
    "for epoch in range(EPOCHS):\n",
    "    langid_model.train() \n",
    "    langid_model.zero_grad()\n",
    "\n",
    "    # Loop over batches\n",
    "    loss = 0\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for batchIdx in range(0, num_batches):\n",
    "        output_scores = langid_model.forward(train_feats_batches[batchIdx])\n",
    "        \n",
    "        output_scores = output_scores.view(BATCH_SIZE * max_len, -1)\n",
    "        flat_labels = train_labels_batches[batchIdx].view(BATCH_SIZE * max_len)\n",
    "        batch_loss = loss_function(output_scores, flat_labels)\n",
    "\n",
    "        predicted_labels = torch.argmax(output_scores, 1)\n",
    "        predicted_labels = predicted_labels.view(BATCH_SIZE, max_len)\n",
    "\n",
    "        # Run backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        langid_model.zero_grad()\n",
    "        loss += batch_loss.item()\n",
    "        # Update the number of correct tags and total tags\n",
    "        for gold_sent, pred_sent in zip(train_labels_batches[batchIdx], predicted_labels):\n",
    "            for gold_label, pred_label in zip(gold_sent, pred_sent):\n",
    "                if gold_label != 0:\n",
    "                    total += 1\n",
    "                    if gold_label == pred_label:\n",
    "                        match+= 1\n",
    "    print('{0: <8}{1: <10}{2}'.format(epoch, '{:.2f}'.format(loss/num_batches), '{:.4f}'.format(match / total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_feats, dev_labels = data2feats(dev_data, token_vocab, label_vocab)\n",
    "# print(langid_model.predict(dev_data[0][))\n",
    "# print(dev_labels[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage for Checklist with test.run\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# dataset = []\n",
    "# new_sentences = []\n",
    "# new_tagged_dataset = []\n",
    "# for sentence in train_data:\n",
    "#     dataset.append(\" \".join(sentence[0]))\n",
    "# pdataset = list(nlp.pipe(dataset))\n",
    "# t_names = Perturb.perturb(pdataset, Perturb.change_names, n=2)\n",
    "\n",
    "# # Tokenize\n",
    "# for sentences in t_names.data:\n",
    "#     for sentence in sentences:\n",
    "#         new_sentences.append(sentence.split())\n",
    "# # Assign NER tags to the generated data\n",
    "# for index, new_sentence in enumerate(new_sentences):\n",
    "#     for sentence in train_data:\n",
    "#         if new_sentence == sentence[0]:\n",
    "#             ner_tag = sentence[1]\n",
    "#     if index % 3 != 0:\n",
    "#         new_tagged_dataset.append((new_sentences[index],ner_tag))\n",
    "\n",
    "\n",
    "# changed_names_feats, dev_labels = data2feats(new_tagged_dataset, token_vocab, label_vocab)\n",
    "\n",
    "# # Use our model\n",
    "# test = INV(**t_names)\n",
    "# test.run(langid_model.predict)\n",
    "# test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'searched', 'all', 'over', 'the', 'internet', ',', 'but', 'I', 'could', 'not', 'find', 'one', 'place', 'in', 'Tampa', 'Bay', 'that', 'sells', 'morcillas', ',', 'also', 'known', 'as', 'blood', 'pudding', ',', 'black', 'pudding', 'and', 'blood', 'sausages', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['I', '<PAD>', 'all', 'over', 'the', 'internet', ',', 'but', 'I', 'could', 'not', 'find', 'one', 'place', 'in', 'Tampa', 'Bay', 'that', '<PAD>', '<PAD>', ',', 'also', 'known', 'as', 'blood', '<PAD>', ',', 'black', '<PAD>', 'and', 'blood', '<PAD>', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Accuracy for dev data: 0.9588\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "def run_eval(feats_batches, labels_batches):\n",
    "    langid_model.eval()\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for sents, labels in zip(feats_batches, labels_batches):\n",
    "        output_scores = langid_model.forward(sents)\n",
    "        predicted_tags  = torch.argmax(output_scores, 2)\n",
    "        for sentence in sents:\n",
    "            sentenceWords = []\n",
    "            for wordIndex in sentence:\n",
    "                sentenceWords.append(token_vocab.getWord(wordIndex.item()))\n",
    "            sentences.append(sentenceWords)\n",
    "        for sentenceTags in predicted_tags:\n",
    "                predictionTagOneSentence = []\n",
    "                for tag in sentenceTags:\n",
    "                    predictionTagOneSentence.append(label_vocab.idx2word[tag.item()])\n",
    "                predictions.append(predictionTagOneSentence)\n",
    "        for goldSent, predSent in zip(labels, predicted_tags):\n",
    "            for goldLabel, predLabel in zip(goldSent, predSent):\n",
    "                if goldLabel.item() != 0:\n",
    "                    total += 1\n",
    "                    if goldLabel.item() == predLabel.item():\n",
    "                        match+= 1\n",
    "    return(match/total)\n",
    "\n",
    "\n",
    "dev_feats, dev_labels = data2feats(dev_data, token_vocab, label_vocab)\n",
    "# print(token_vocab.getWord(dev_feats[0][0].item()))\n",
    "num_batches_dev = int(len(dev_feats)/BATCH_SIZE)\n",
    "\n",
    "dev_feats_batches = dev_feats[:BATCH_SIZE*num_batches_dev].view(num_batches_dev, BATCH_SIZE, max_len)\n",
    "dev_labels_batches = dev_labels[:BATCH_SIZE*num_batches_dev].view(num_batches_dev, BATCH_SIZE, max_len)\n",
    "score = run_eval(dev_feats_batches, dev_labels_batches)\n",
    "\n",
    "print(dev_data[1][0])\n",
    "print(dev_data[1][1])\n",
    "print('')\n",
    "print(sentences[1])\n",
    "print(predictions[1])\n",
    "print('Accuracy for dev data: {:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "dataset = []\n",
    "for sentence in dev_data:\n",
    "    dataset.append(\" \".join(sentence[0]))\n",
    "pdataset = list(nlp.pipe(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n",
      "['Which', 'of', 'these', 'do', 'you', 'like', ':', 'McDonald', 's', ',', 'Burger', 'King', ',', 'Taco', 'Bell', ',', 'Wendy', 's', '?']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "\n",
      "['<PAD>', 'is', 'pregnant', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\u001b[32mAccuracy for changed names data: \u001b[0m 0.9013\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'O', 'B-LOC', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "new_sentences = []\n",
    "new_tagged_dataset = []\n",
    "# nsamples = how many 'name' sentences we want to take into account\n",
    "# n = represents number of sentences that we want to generate for each 'name' sentence\n",
    "t_names = Perturb.perturb(pdataset, Perturb.change_names, n=2)\n",
    "original_sentences = []\n",
    "for sentences in t_names.data:\n",
    "    original_sentences.append(sentences[0])\n",
    "# Tokenize\n",
    "for sentences in t_names.data:\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append(sentence.split())\n",
    "# Assign NER tags to the generated data\n",
    "for index, new_sentence in enumerate(new_sentences):\n",
    "    for sentence in dev_data:\n",
    "        if new_sentence == sentence[0]:\n",
    "            ner_tag = sentence[1]\n",
    "    if index % 3 != 0:\n",
    "        new_tagged_dataset.append((new_sentences[index],ner_tag))\n",
    "\n",
    "# Create gold labels file: index<TAB>word<TAB>label. \n",
    "with open('changed_names_data.iob2', 'w') as f:\n",
    "    for sentence, tag in new_tagged_dataset:\n",
    "        for index, (token, pred) in enumerate(zip(sentence, tag)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "sentences = []\n",
    "predictions = []\n",
    "print(len(new_tagged_dataset))\n",
    "\n",
    "changed_names_feats, dev_labels = data2feats(new_tagged_dataset, token_vocab, label_vocab)\n",
    "num_batches_changed_names = int(len(changed_names_feats)/BATCH_SIZE)\n",
    "\n",
    "changed_names_feats_batches = changed_names_feats[:BATCH_SIZE*num_batches_changed_names].view(num_batches_changed_names, BATCH_SIZE, max_len)\n",
    "changed_names_labels_batches = dev_labels[:BATCH_SIZE*num_batches_changed_names].view(num_batches_changed_names, BATCH_SIZE, max_len)\n",
    "score = run_eval(changed_names_feats_batches, changed_names_labels_batches)\n",
    "\n",
    "print(new_tagged_dataset[1][0])\n",
    "print(new_tagged_dataset[1][1])\n",
    "print('')\n",
    "print(sentences[5])\n",
    "print(predictions[5])\n",
    "print('\\033[32mAccuracy for changed names data: \\033[0m {:.4f}'.format(score))\n",
    "\n",
    "with open('predictions_changed_names_data.iob2', 'w') as f:\n",
    "    for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "        for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "\n",
    "predictionsNames = read_iob2_file('./predictions_changed_names_data.iob2')\n",
    "\n",
    "print(new_tagged_dataset[0][1])\n",
    "print(predictionsNames[0][1])\n",
    "\n",
    "# python3 span_f1.py changed_names_data.iob2 predictions_changed_names_data.iob2 <- run this in terminal to get span f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "['There', 'are', 'way', 'more', 'stranger', 'names', 'in', 'the', 'U.S', 'for', 'areas', 'than', 'Bakersfield', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-LOC', 'O']\n",
      "\n",
      "['How', 'about', '<PAD>', '<PAD>', 'or', 'other', '<PAD>', 'from', 'that', 'area', 'of', '<PAD>', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\u001b[32mAccuracy for changed location data: \u001b[0m 0.9179\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'B-LOC', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "new_sentences = []\n",
    "new_tagged_dataset = []\n",
    "# nsamples = how many 'name' sentences we want to take into account\n",
    "# n = represents number of sentences that we want to generate for each 'name' sentence\n",
    "t_location = Perturb.perturb(pdataset, Perturb.change_location, n=2)\n",
    "original_sentences = []\n",
    "for sentences in t_location.data:\n",
    "    original_sentences.append(sentences[0])\n",
    "# Tokenize\n",
    "for sentences in t_location.data:\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append(sentence.split())\n",
    "# Assign NER tags to the generated data\n",
    "for index, new_sentence in enumerate(new_sentences):\n",
    "    for sentence in dev_data:\n",
    "        if new_sentence == sentence[0]:\n",
    "            ner_tag = sentence[1]\n",
    "    if index % 3 != 0:\n",
    "        new_tagged_dataset.append((new_sentences[index],ner_tag))\n",
    "\n",
    "# Create gold labels file: index<TAB>word<TAB>label. \n",
    "with open('changed_location_data.iob2', 'w') as f:\n",
    "    for sentence, tag in new_tagged_dataset:\n",
    "        for index, (token, pred) in enumerate(zip(sentence, tag)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "sentences = []\n",
    "predictions = []\n",
    "print(len(new_tagged_dataset))\n",
    "\n",
    "changed_location_feats, dev_labels = data2feats(new_tagged_dataset, token_vocab, label_vocab)\n",
    "num_batches_changed_location = int(len(changed_location_feats)/BATCH_SIZE)\n",
    "\n",
    "changed_location_feats_batches = changed_location_feats[:BATCH_SIZE*num_batches_changed_location].view(num_batches_changed_location, BATCH_SIZE, max_len)\n",
    "changed_location_labels_batches = dev_labels[:BATCH_SIZE*num_batches_changed_location].view(num_batches_changed_location, BATCH_SIZE, max_len)\n",
    "score = run_eval(changed_location_feats_batches, changed_location_labels_batches)\n",
    "\n",
    "print(new_tagged_dataset[1][0])\n",
    "print(new_tagged_dataset[1][1])\n",
    "print('')\n",
    "print(sentences[5])\n",
    "print(predictions[5])\n",
    "print('\\033[32mAccuracy for changed location data: \\033[0m {:.4f}'.format(score))\n",
    "\n",
    "with open('predictions_changed_location_data.iob2', 'w') as f:\n",
    "    for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "        for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "\n",
    "predictionsLocation = read_iob2_file('./predictions_changed_location_data.iob2')\n",
    "\n",
    "print(new_tagged_dataset[0][1])\n",
    "print(predictionsLocation[0][1])\n",
    "\n",
    "# python3 span_f1.py changed_location_data.iob2 predictions_changed_location_data.iob2 <- run this in terminal to get span f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n",
      "['2', 'cup', 'of', 'empanadas']\n",
      "['O', 'O', 'O', 'O']\n",
      "\n",
      "['2', 'cup', 'of', 'other', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\u001b[32mAccuracy for changed number data: \u001b[0m 0.9379\n",
      "['O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "new_sentences = []\n",
    "new_tagged_dataset = []\n",
    "# nsamples = how many 'name' sentences we want to take into account\n",
    "# n = represents number of sentences that we want to generate for each 'name' sentence\n",
    "t_number = Perturb.perturb(pdataset, Perturb.change_number, n=2)\n",
    "original_sentences = []\n",
    "for sentences in t_number.data:\n",
    "    original_sentences.append(sentences[0])\n",
    "# Tokenize\n",
    "for sentences in t_number.data:\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append(sentence.split())\n",
    "# Assign NER tags to the generated data\n",
    "for index, new_sentence in enumerate(new_sentences):\n",
    "    for sentence in dev_data:\n",
    "        if new_sentence == sentence[0]:\n",
    "            ner_tag = sentence[1]\n",
    "    if index % 3 != 0:\n",
    "        new_tagged_dataset.append((new_sentences[index],ner_tag))\n",
    "\n",
    "# Create gold labels file: index<TAB>word<TAB>label. \n",
    "with open('changed_number_data.iob2', 'w') as f:\n",
    "    for sentence, tag in new_tagged_dataset:\n",
    "        for index, (token, pred) in enumerate(zip(sentence, tag)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "sentences = []\n",
    "predictions = []\n",
    "print(len(new_tagged_dataset))\n",
    "\n",
    "changed_number_feats, dev_labels = data2feats(new_tagged_dataset, token_vocab, label_vocab)\n",
    "num_batches_changed_number = int(len(changed_number_feats)/BATCH_SIZE)\n",
    "\n",
    "changed_number_feats_batches = changed_number_feats[:BATCH_SIZE*num_batches_changed_number].view(num_batches_changed_number, BATCH_SIZE, max_len)\n",
    "changed_number_labels_batches = dev_labels[:BATCH_SIZE*num_batches_changed_number].view(num_batches_changed_number, BATCH_SIZE, max_len)\n",
    "score = run_eval(changed_number_feats_batches, changed_number_labels_batches)\n",
    "\n",
    "print(new_tagged_dataset[1][0])\n",
    "print(new_tagged_dataset[1][1])\n",
    "print('')\n",
    "print(sentences[5])\n",
    "print(predictions[5])\n",
    "print('\\033[32mAccuracy for changed number data: \\033[0m {:.4f}'.format(score))\n",
    "\n",
    "with open('predictions_changed_number_data.iob2', 'w') as f:\n",
    "    for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "        for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "\n",
    "predictionsNumber = read_iob2_file('./predictions_changed_number_data.iob2')\n",
    "\n",
    "print(new_tagged_dataset[0][1])\n",
    "print(predictionsNumber[0][1])\n",
    "\n",
    "# python3 span_f1.py changed_number_data.iob2 predictions_changed_number_data.iob2 <- run this in terminal to get span f1 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
