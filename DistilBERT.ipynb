{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from checklist.perturb import Perturb\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "import os.path\n",
    "import sklearn\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iob2_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "train_data= read_iob2_file('data//en_ewt-ud-train.iob2')\n",
    "dev_data = read_iob2_file('data//en_ewt-ud-dev.iob2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "PAD = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', 'O', 'B-LOC', 'I-LOC', 'B-PER', 'B-ORG', 'I-ORG', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, pad_unk):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "\n",
    "max_len = max([len(x[0]) for x in train_data ])\n",
    "\n",
    "# Create vocabularies for both the tokens\n",
    "# and the tags\n",
    "token_vocab = Vocab(PAD)\n",
    "label_vocab = Vocab(PAD)\n",
    "id_to_token = [PAD]\n",
    "\n",
    "for tokens, tags in train_data:\n",
    "    for token in tokens:\n",
    "        token_vocab.getIdx(token, True)\n",
    "    for tag in tags:\n",
    "        label_vocab.getIdx(tag, True)\n",
    "\n",
    "NWORDS = len(token_vocab.idx2word)\n",
    "NTAGS = len(label_vocab.idx2word)\n",
    "print(label_vocab.idx2word)\n",
    "\n",
    "# convert text data with labels to indices\n",
    "def data2feats(inputData, word_vocab, label_vocab):\n",
    "    feats = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    labels = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    for sentPos, sent in enumerate(inputData):\n",
    "        for wordPos, word in enumerate(sent[0][:max_len]):\n",
    "            wordIdx = word_vocab.getIdx(word)\n",
    "            feats[sentPos][wordPos] = wordIdx\n",
    "        for labelPos, label in enumerate(sent[1][:max_len]):\n",
    "            labelIdx = label_vocab.getIdx(label)\n",
    "            labels[sentPos][labelPos] = labelIdx\n",
    "    return feats, labels\n",
    "\n",
    "train_features, train_labels = data2feats(train_data, token_vocab, label_vocab)\n",
    "dev_feats, dev_labels = data2feats(dev_data, token_vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAG DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x32441abd0>,\n",
       "  <matplotlib.axis.XTick at 0x3242d05d0>,\n",
       "  <matplotlib.axis.XTick at 0x3244185d0>,\n",
       "  <matplotlib.axis.XTick at 0x32444fad0>,\n",
       "  <matplotlib.axis.XTick at 0x324459c90>,\n",
       "  <matplotlib.axis.XTick at 0x32445be90>],\n",
       " [Text(0, 0, 'B-LOC'),\n",
       "  Text(1, 0, 'I-LOC'),\n",
       "  Text(2, 0, 'B-PER'),\n",
       "  Text(3, 0, 'I-PER'),\n",
       "  Text(4, 0, 'B-ORG'),\n",
       "  Text(5, 0, 'I-ORG')])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = {\n",
    "    # \"O\": 0,\n",
    "    \"B-LOC\": 0,\n",
    "    \"I-LOC\": 0,\n",
    "    \"B-PER\": 0,\n",
    "    \"I-PER\": 0,\n",
    "    \"B-ORG\": 0,\n",
    "    \"I-ORG\": 0,\n",
    "}\n",
    "\n",
    "for sentence in train_data:\n",
    "    for tag in sentence[1]:\n",
    "        if tag == 'O':\n",
    "            pass\n",
    "            # counts['O'] += 1\n",
    "        elif tag == 'B-LOC':\n",
    "            counts['B-LOC'] += 1\n",
    "        elif tag == 'I-LOC':\n",
    "            counts['I-LOC'] += 1\n",
    "        elif tag == 'B-PER':\n",
    "            counts['B-PER'] += 1\n",
    "        elif tag == 'I-PER':\n",
    "            counts['I-PER'] += 1\n",
    "        elif tag == 'B-ORG':\n",
    "            counts['B-ORG'] += 1\n",
    "        elif tag == 'I-ORG':\n",
    "            counts['I-ORG'] += 1\n",
    "        else:\n",
    "            print('unsupported tag')\n",
    "    \n",
    "plt.bar(range(len(counts)), list(counts.values()), align='center')\n",
    "plt.xticks(range(len(counts)), list(counts.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"nlp-project\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2073, 1999, 1996, 2088, 2003, 1045, 19696, 9759, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 1, 1, 1, 1, 2, 1]}\n",
      "{'input_ids': [101, 2073, 2064, 1045, 2131, 22822, 6895, 25816, 1999, 9925, 3016, 1010, 1045, 2097, 2066, 1996, 23157, 15830, 2828, 1010, 2021, 1045, 2097, 2000, 3046, 2178, 2015, 3531, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "def align_labels(tokenized_input, labels):\n",
    "  word_ids = tokenized_input.word_ids()\n",
    "  aligned_labels = []\n",
    "  previous_word_idx = None\n",
    "  for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == tokenizer.pad_token_id:\n",
    "      aligned_labels.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "      aligned_labels.append(labels[word_idx])\n",
    "    else:\n",
    "      aligned_labels.append(-100)\n",
    "    previous_word_idx = word_idx\n",
    "  return aligned_labels\n",
    "\n",
    "# TRAIN DATA\n",
    "tokenized_train = []\n",
    "for sentence, labels in train_data:\n",
    "  tokenized_input = tokenizer(sentence, is_split_into_words=True)\n",
    "  labelsIndices = []\n",
    "  for label in labels:\n",
    "    labelsIndices.append(label_vocab.getIdx(label))\n",
    "  tokenized_input['labels'] = labelsIndices\n",
    "  tokenized_train.append(tokenized_input)\n",
    "print(tokenized_train[0])\n",
    "\n",
    "for tokenized_input in tokenized_train:\n",
    "  aligned_labels = align_labels(tokenized_input, tokenized_input['labels'])\n",
    "  tokenized_input['labels'] = aligned_labels\n",
    "\n",
    "# DEV DATA\n",
    "tokenized_dev = []\n",
    "for sentence, labels in dev_data:\n",
    "  tokenized_input = tokenizer(sentence, is_split_into_words=True)\n",
    "  labelsIndices = []\n",
    "  for label in labels:\n",
    "    labelsIndices.append(label_vocab.getIdx(label))\n",
    "  tokenized_input['labels'] = labelsIndices\n",
    "  tokenized_dev.append(tokenized_input)\n",
    "print(tokenized_dev[0])\n",
    "\n",
    "for tokenized_input in tokenized_dev:\n",
    "  aligned_labels = align_labels(tokenized_input, tokenized_input['labels'])\n",
    "  tokenized_input['labels'] = aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataCollatorForTokenClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorForTokenClassification(tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m      3\u001b[0m seqeval \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mseqeval\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataCollatorForTokenClassification' is not defined"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"<PAD>\",\n",
    "    1: \"O\",\n",
    "    2: \"B-LOC\",\n",
    "    3: \"I-LOC\",\n",
    "    4: \"B-PER\",\n",
    "    5: \"I-PER\",\n",
    "    6: \"B-ORG\",\n",
    "    7: \"I-ORG\",\n",
    "}\n",
    "label2id = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"O\": 1,\n",
    "    \"B-LOC\": 2,\n",
    "    \"I-LOC\": 3,\n",
    "    \"B-PER\": 4,\n",
    "    \"I-PER\": 5,\n",
    "    \"B-ORG\": 6,\n",
    "    \"I-ORG\": 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=NTAGS, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrichardkentos\u001b[0m (\u001b[33mrike\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/richardkentos/ITU/nlp/project/NLP/wandb/run-20240520_120029-8we40xxt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rike/nlp-project/runs/8we40xxt/workspace' target=\"_blank\">vivid-bird-2</a></strong> to <a href='https://wandb.ai/rike/nlp-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rike/nlp-project' target=\"_blank\">https://wandb.ai/rike/nlp-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rike/nlp-project/runs/8we40xxt/workspace' target=\"_blank\">https://wandb.ai/rike/nlp-project/runs/8we40xxt/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00ca38202cf4a1291b9453bda1a860b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1216, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 2.08 GB, other allocations: 6.95 GB, max allowed: 9.07 GB). Tried to allocate 89.42 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtesting_bert\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     report_to\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwandb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# push_to_hub=True,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[39m# compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     26\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1917\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   1912\u001b[0m             model\u001b[39m.\u001b[39mparameters(),\n\u001b[1;32m   1913\u001b[0m             args\u001b[39m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   1914\u001b[0m         )\n\u001b[1;32m   1916\u001b[0m \u001b[39m# Optimizer step\u001b[39;00m\n\u001b[0;32m-> 1917\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m   1918\u001b[0m optimizer_was_run \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   1919\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   1920\u001b[0m     \u001b[39m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/accelerate/optimizer.py:170\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerate_step_called \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep(closure)\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator_state\u001b[39m.\u001b[39mdistributed_type \u001b[39m==\u001b[39m DistributedType\u001b[39m.\u001b[39mXLA:\n\u001b[1;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_state\u001b[39m.\u001b[39mis_xla_gradients_synced \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:187\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    174\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    176\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    177\u001b[0m         group,\n\u001b[1;32m    178\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m         state_steps,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[0;32m--> 187\u001b[0m     adamw(\n\u001b[1;32m    188\u001b[0m         params_with_grad,\n\u001b[1;32m    189\u001b[0m         grads,\n\u001b[1;32m    190\u001b[0m         exp_avgs,\n\u001b[1;32m    191\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    192\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    193\u001b[0m         state_steps,\n\u001b[1;32m    194\u001b[0m         amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[1;32m    195\u001b[0m         beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[1;32m    196\u001b[0m         beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[1;32m    197\u001b[0m         lr\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    198\u001b[0m         weight_decay\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    199\u001b[0m         eps\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    200\u001b[0m         maximize\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    201\u001b[0m         foreach\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mforeach\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    202\u001b[0m         capturable\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    203\u001b[0m         differentiable\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    204\u001b[0m         fused\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mfused\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    205\u001b[0m         grad_scale\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgrad_scale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    206\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    207\u001b[0m         has_complex\u001b[39m=\u001b[39mhas_complex,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:339\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 339\u001b[0m func(\n\u001b[1;32m    340\u001b[0m     params,\n\u001b[1;32m    341\u001b[0m     grads,\n\u001b[1;32m    342\u001b[0m     exp_avgs,\n\u001b[1;32m    343\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    344\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    345\u001b[0m     state_steps,\n\u001b[1;32m    346\u001b[0m     amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[1;32m    347\u001b[0m     beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[1;32m    348\u001b[0m     beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[1;32m    349\u001b[0m     lr\u001b[39m=\u001b[39mlr,\n\u001b[1;32m    350\u001b[0m     weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[1;32m    351\u001b[0m     eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m    352\u001b[0m     maximize\u001b[39m=\u001b[39mmaximize,\n\u001b[1;32m    353\u001b[0m     capturable\u001b[39m=\u001b[39mcapturable,\n\u001b[1;32m    354\u001b[0m     differentiable\u001b[39m=\u001b[39mdifferentiable,\n\u001b[1;32m    355\u001b[0m     grad_scale\u001b[39m=\u001b[39mgrad_scale,\n\u001b[1;32m    356\u001b[0m     found_inf\u001b[39m=\u001b[39mfound_inf,\n\u001b[1;32m    357\u001b[0m     has_complex\u001b[39m=\u001b[39mhas_complex,\n\u001b[1;32m    358\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:470\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    468\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    469\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 470\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    472\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[1;32m    474\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 2.08 GB, other allocations: 6.95 GB, max allowed: 9.07 GB). Tried to allocate 89.42 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"testing_bert\",\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4, # with 16 f1 0.66\n",
    "    per_device_eval_batch_size=4, # with 16 f1 0.66\n",
    "    num_train_epochs=5, # with 2 f1 0.66\n",
    "    weight_decay=0.0, # with 0.01 f1 0.66\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our finetuned model\n",
    "fine_tuned = AutoModelForTokenClassification.from_pretrained(\"full_distilBERT/checkpoint-1568/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [\"The Golden State Warriors are an American professional basketball team based in San Francisco.\", \"My name is Richard Kentos\"]\n",
    "\n",
    "# from transformers import pipeline\n",
    "# classifier = pipeline(\"ner\", model=\"testing_bert/checkpoint-376/\")\n",
    "\n",
    "# for text in classifier(text):\n",
    "#     print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(path):\n",
    "    \"\"\"\n",
    "    get sentences from conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if line[:6] == '# text':\n",
    "                data.append(line[9:])\n",
    "    return data\n",
    "\n",
    "# train_data= read_iob2_file('data//en_ewt-ud-train.iob2')\n",
    "dev_sentences = get_sentences('data//en_ewt-ud-dev.iob2')\n",
    "dev_sents_tokenized = []\n",
    "for sentence in dev_data:\n",
    "    dev_sents_tokenized.append(sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dev data: 0.8983\n",
      "['where', 'can', 'I', 'get', 'morcillas', 'in', 'tampa', 'bay', ',', 'I', 'will', 'like', 'the', 'argentinian', 'type', ',', 'but', 'I', 'will', 'to', 'try', 'anothers', 'please', '?']\n",
      "2001\n"
     ]
    }
   ],
   "source": [
    "predictionTags = []\n",
    "\n",
    "def run_eval(sentences, gold_labels):\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for sents, labels in zip(sentences, gold_labels):\n",
    "        inputs = tokenizer(sents, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=True)\n",
    "        predictionTagOneSentence = []\n",
    "        with torch.no_grad():\n",
    "            word_ids = inputs.word_ids()\n",
    "            # tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].flatten())\n",
    "            logits = fine_tuned(**inputs).logits\n",
    "            predictions = torch.argmax(logits, dim=-1).flatten()\n",
    "            previous_word_idx = None\n",
    "            for idx, word_idx in enumerate(word_ids):\n",
    "                if previous_word_idx != word_idx and word_idx is not None:\n",
    "                    predictionTagOneSentence.append(label_vocab.idx2word[predictions[idx].item()])\n",
    "                previous_word_idx = word_idx\n",
    "        predictionTags.append(predictionTagOneSentence)\n",
    "\n",
    "        # Loop through gold labels\n",
    "        for goldLabel, predLabel in zip(labels, predictions):\n",
    "            if goldLabel.item() != 0:\n",
    "                total += 1\n",
    "                if goldLabel.item() == predLabel.item():\n",
    "                    match+= 1\n",
    "    return(match/total)\n",
    "    \n",
    "\n",
    "score = run_eval(dev_sents_tokenized[:len(dev_sents_tokenized)+1], dev_labels[:len(dev_labels)+1])\n",
    "\n",
    "print('Accuracy for dev data: {:.4f}'.format(score))\n",
    "\n",
    "print(dev_data[0][0])\n",
    "print(len(predictionTags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'new_bert_predictions_dev.iob2'), 'w') as f:\n",
    "    for sent_tokens, sent_preds in zip(dev_sents_tokenized[:len(dev_sents_tokenized)+1], predictionTags[:len(predictionTags)+1]):\n",
    "        for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# python3 span_f1.py data/new_bert_predictions_dev.iob2 data/en_ewt-ud-dev.iob2  <- run this in terminal to get span f1 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
