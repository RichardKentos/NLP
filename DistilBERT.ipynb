{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from checklist.perturb import Perturb\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "import os.path\n",
    "import sklearn\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iob2_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "train_data= read_iob2_file('data//en_ewt-ud-train.iob2')\n",
    "dev_data = read_iob2_file('data//en_ewt-ud-dev.iob2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "PAD = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', 'O', 'B-LOC', 'I-LOC', 'B-PER', 'B-ORG', 'I-ORG', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, pad_unk):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "\n",
    "max_len = max([len(x[0]) for x in train_data ])\n",
    "\n",
    "# Create vocabularies for both the tokens\n",
    "# and the tags\n",
    "token_vocab = Vocab(PAD)\n",
    "label_vocab = Vocab(PAD)\n",
    "id_to_token = [PAD]\n",
    "\n",
    "for tokens, tags in train_data:\n",
    "    for token in tokens:\n",
    "        token_vocab.getIdx(token, True)\n",
    "    for tag in tags:\n",
    "        label_vocab.getIdx(tag, True)\n",
    "\n",
    "NWORDS = len(token_vocab.idx2word)\n",
    "NTAGS = len(label_vocab.idx2word)\n",
    "print(label_vocab.idx2word)\n",
    "\n",
    "# convert text data with labels to indices\n",
    "def data2feats(inputData, word_vocab, label_vocab):\n",
    "    feats = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    labels = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    for sentPos, sent in enumerate(inputData):\n",
    "        for wordPos, word in enumerate(sent[0][:max_len]):\n",
    "            wordIdx = word_vocab.getIdx(word)\n",
    "            feats[sentPos][wordPos] = wordIdx\n",
    "        for labelPos, label in enumerate(sent[1][:max_len]):\n",
    "            labelIdx = label_vocab.getIdx(label)\n",
    "            labels[sentPos][labelPos] = labelIdx\n",
    "    return feats, labels\n",
    "\n",
    "train_features, train_labels = data2feats(train_data, token_vocab, label_vocab)\n",
    "dev_feats, dev_labels = data2feats(dev_data, token_vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAG DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 194219, 'B-LOC': 2712, 'I-LOC': 877, 'B-PER': 2874, 'I-PER': 1294, 'B-ORG': 1436, 'I-ORG': 1167}\n",
      "204579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x15008a7d0>,\n",
       "  <matplotlib.axis.XTick at 0x15007dc90>,\n",
       "  <matplotlib.axis.XTick at 0x15005b790>,\n",
       "  <matplotlib.axis.XTick at 0x150004850>,\n",
       "  <matplotlib.axis.XTick at 0x32348fed0>,\n",
       "  <matplotlib.axis.XTick at 0x1500c90d0>,\n",
       "  <matplotlib.axis.XTick at 0x1500cb2d0>],\n",
       " [Text(0, 0, 'O'),\n",
       "  Text(1, 0, 'B-LOC'),\n",
       "  Text(2, 0, 'I-LOC'),\n",
       "  Text(3, 0, 'B-PER'),\n",
       "  Text(4, 0, 'I-PER'),\n",
       "  Text(5, 0, 'B-ORG'),\n",
       "  Text(6, 0, 'I-ORG')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = {\n",
    "    \"O\": 0,\n",
    "    \"B-LOC\": 0,\n",
    "    \"I-LOC\": 0,\n",
    "    \"B-PER\": 0,\n",
    "    \"I-PER\": 0,\n",
    "    \"B-ORG\": 0,\n",
    "    \"I-ORG\": 0,\n",
    "}\n",
    "\n",
    "for sentence in train_data:\n",
    "    for tag in sentence[1]:\n",
    "        if tag == 'O':\n",
    "            counts['O'] += 1\n",
    "        elif tag == 'B-LOC':\n",
    "            counts['B-LOC'] += 1\n",
    "        elif tag == 'I-LOC':\n",
    "            counts['I-LOC'] += 1\n",
    "        elif tag == 'B-PER':\n",
    "            counts['B-PER'] += 1\n",
    "        elif tag == 'I-PER':\n",
    "            counts['I-PER'] += 1\n",
    "        elif tag == 'B-ORG':\n",
    "            counts['B-ORG'] += 1\n",
    "        elif tag == 'I-ORG':\n",
    "            counts['I-ORG'] += 1\n",
    "        else:\n",
    "            print('unsupported tag')\n",
    "    \n",
    "print(counts)\n",
    "total = 0\n",
    "for value in counts.values():\n",
    "    total += value\n",
    "print(total)\n",
    "plt.bar(range(len(counts)), list(counts.values()), align='center')\n",
    "plt.xticks(range(len(counts)), list(counts.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"nlp-project\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2073, 1999, 1996, 2088, 2003, 1045, 19696, 9759, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 1, 1, 1, 1, 2, 1]}\n",
      "{'input_ids': [101, 2073, 2064, 1045, 2131, 22822, 6895, 25816, 1999, 9925, 3016, 1010, 1045, 2097, 2066, 1996, 23157, 15830, 2828, 1010, 2021, 1045, 2097, 2000, 3046, 2178, 2015, 3531, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "def align_labels(tokenized_input, labels):\n",
    "  word_ids = tokenized_input.word_ids()\n",
    "  aligned_labels = []\n",
    "  previous_word_idx = None\n",
    "  for word_idx in word_ids:\n",
    "    if word_idx is None or word_idx == tokenizer.pad_token_id:\n",
    "      aligned_labels.append(-100)\n",
    "    elif word_idx != previous_word_idx:\n",
    "      aligned_labels.append(labels[word_idx])\n",
    "    else:\n",
    "      aligned_labels.append(-100)\n",
    "    previous_word_idx = word_idx\n",
    "  return aligned_labels\n",
    "\n",
    "# TRAIN DATA\n",
    "tokenized_train = []\n",
    "for sentence, labels in train_data:\n",
    "  tokenized_input = tokenizer(sentence, is_split_into_words=True)\n",
    "  labelsIndices = []\n",
    "  for label in labels:\n",
    "    labelsIndices.append(label_vocab.getIdx(label))\n",
    "  tokenized_input['labels'] = labelsIndices\n",
    "  tokenized_train.append(tokenized_input)\n",
    "print(tokenized_train[0])\n",
    "\n",
    "for tokenized_input in tokenized_train:\n",
    "  aligned_labels = align_labels(tokenized_input, tokenized_input['labels'])\n",
    "  tokenized_input['labels'] = aligned_labels\n",
    "\n",
    "# DEV DATA\n",
    "tokenized_dev = []\n",
    "for sentence, labels in dev_data:\n",
    "  tokenized_input = tokenizer(sentence, is_split_into_words=True)\n",
    "  labelsIndices = []\n",
    "  for label in labels:\n",
    "    labelsIndices.append(label_vocab.getIdx(label))\n",
    "  tokenized_input['labels'] = labelsIndices\n",
    "  tokenized_dev.append(tokenized_input)\n",
    "print(tokenized_dev[0])\n",
    "\n",
    "for tokenized_input in tokenized_dev:\n",
    "  aligned_labels = align_labels(tokenized_input, tokenized_input['labels'])\n",
    "  tokenized_input['labels'] = aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataCollatorForTokenClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorForTokenClassification(tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m      3\u001b[0m seqeval \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mseqeval\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataCollatorForTokenClassification' is not defined"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"<PAD>\",\n",
    "    1: \"O\",\n",
    "    2: \"B-LOC\",\n",
    "    3: \"I-LOC\",\n",
    "    4: \"B-PER\",\n",
    "    5: \"I-PER\",\n",
    "    6: \"B-ORG\",\n",
    "    7: \"I-ORG\",\n",
    "}\n",
    "label2id = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"O\": 1,\n",
    "    \"B-LOC\": 2,\n",
    "    \"I-LOC\": 3,\n",
    "    \"B-PER\": 4,\n",
    "    \"I-PER\": 5,\n",
    "    \"B-ORG\": 6,\n",
    "    \"I-ORG\": 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=NTAGS, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrichardkentos\u001b[0m (\u001b[33mrike\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/richardkentos/ITU/nlp/project/NLP/wandb/run-20240520_120029-8we40xxt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rike/nlp-project/runs/8we40xxt/workspace' target=\"_blank\">vivid-bird-2</a></strong> to <a href='https://wandb.ai/rike/nlp-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rike/nlp-project' target=\"_blank\">https://wandb.ai/rike/nlp-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rike/nlp-project/runs/8we40xxt/workspace' target=\"_blank\">https://wandb.ai/rike/nlp-project/runs/8we40xxt/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00ca38202cf4a1291b9453bda1a860b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1216, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 2.08 GB, other allocations: 6.95 GB, max allowed: 9.07 GB). Tried to allocate 89.42 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtesting_bert\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     report_to\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwandb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39m# push_to_hub=True,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[39m# compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     26\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1917\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   1912\u001b[0m             model\u001b[39m.\u001b[39mparameters(),\n\u001b[1;32m   1913\u001b[0m             args\u001b[39m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   1914\u001b[0m         )\n\u001b[1;32m   1916\u001b[0m \u001b[39m# Optimizer step\u001b[39;00m\n\u001b[0;32m-> 1917\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m   1918\u001b[0m optimizer_was_run \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   1919\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   1920\u001b[0m     \u001b[39m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/accelerate/optimizer.py:170\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerate_step_called \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep(closure)\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator_state\u001b[39m.\u001b[39mdistributed_type \u001b[39m==\u001b[39m DistributedType\u001b[39m.\u001b[39mXLA:\n\u001b[1;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_state\u001b[39m.\u001b[39mis_xla_gradients_synced \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:187\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    174\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    176\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    177\u001b[0m         group,\n\u001b[1;32m    178\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m         state_steps,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[0;32m--> 187\u001b[0m     adamw(\n\u001b[1;32m    188\u001b[0m         params_with_grad,\n\u001b[1;32m    189\u001b[0m         grads,\n\u001b[1;32m    190\u001b[0m         exp_avgs,\n\u001b[1;32m    191\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    192\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    193\u001b[0m         state_steps,\n\u001b[1;32m    194\u001b[0m         amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[1;32m    195\u001b[0m         beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[1;32m    196\u001b[0m         beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[1;32m    197\u001b[0m         lr\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    198\u001b[0m         weight_decay\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    199\u001b[0m         eps\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    200\u001b[0m         maximize\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    201\u001b[0m         foreach\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mforeach\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    202\u001b[0m         capturable\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    203\u001b[0m         differentiable\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    204\u001b[0m         fused\u001b[39m=\u001b[39mgroup[\u001b[39m\"\u001b[39m\u001b[39mfused\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    205\u001b[0m         grad_scale\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgrad_scale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    206\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    207\u001b[0m         has_complex\u001b[39m=\u001b[39mhas_complex,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:339\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 339\u001b[0m func(\n\u001b[1;32m    340\u001b[0m     params,\n\u001b[1;32m    341\u001b[0m     grads,\n\u001b[1;32m    342\u001b[0m     exp_avgs,\n\u001b[1;32m    343\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    344\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    345\u001b[0m     state_steps,\n\u001b[1;32m    346\u001b[0m     amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[1;32m    347\u001b[0m     beta1\u001b[39m=\u001b[39mbeta1,\n\u001b[1;32m    348\u001b[0m     beta2\u001b[39m=\u001b[39mbeta2,\n\u001b[1;32m    349\u001b[0m     lr\u001b[39m=\u001b[39mlr,\n\u001b[1;32m    350\u001b[0m     weight_decay\u001b[39m=\u001b[39mweight_decay,\n\u001b[1;32m    351\u001b[0m     eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m    352\u001b[0m     maximize\u001b[39m=\u001b[39mmaximize,\n\u001b[1;32m    353\u001b[0m     capturable\u001b[39m=\u001b[39mcapturable,\n\u001b[1;32m    354\u001b[0m     differentiable\u001b[39m=\u001b[39mdifferentiable,\n\u001b[1;32m    355\u001b[0m     grad_scale\u001b[39m=\u001b[39mgrad_scale,\n\u001b[1;32m    356\u001b[0m     found_inf\u001b[39m=\u001b[39mfound_inf,\n\u001b[1;32m    357\u001b[0m     has_complex\u001b[39m=\u001b[39mhas_complex,\n\u001b[1;32m    358\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/optim/adamw.py:470\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    468\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    469\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 470\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    472\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[1;32m    474\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 2.08 GB, other allocations: 6.95 GB, max allowed: 9.07 GB). Tried to allocate 89.42 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"testing_bert\",\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4, # with 16 f1 0.66\n",
    "    per_device_eval_batch_size=4, # with 16 f1 0.66\n",
    "    num_train_epochs=5, # with 2 f1 0.66\n",
    "    weight_decay=0.0, # with 0.01 f1 0.66\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our finetuned model\n",
    "fine_tuned = AutoModelForTokenClassification.from_pretrained(\"full_distilBERT/checkpoint-1568/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [\"The Golden State Warriors are an American professional basketball team based in San Francisco.\", \"My name is Richard Kentos\"]\n",
    "\n",
    "# from transformers import pipeline\n",
    "# classifier = pipeline(\"ner\", model=\"testing_bert/checkpoint-376/\")\n",
    "\n",
    "# for text in classifier(text):\n",
    "#     print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(path):\n",
    "    \"\"\"\n",
    "    get sentences from conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if line[:6] == '# text':\n",
    "                data.append(line[9:])\n",
    "    return data\n",
    "\n",
    "# train_data= read_iob2_file('data//en_ewt-ud-train.iob2')\n",
    "dev_sentences = get_sentences('data//en_ewt-ud-dev.iob2')\n",
    "dev_sents_tokenized = []\n",
    "for sentence in dev_data:\n",
    "    dev_sents_tokenized.append(sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dev data: 0.8983\n",
      "['where', 'can', 'I', 'get', 'morcillas', 'in', 'tampa', 'bay', ',', 'I', 'will', 'like', 'the', 'argentinian', 'type', ',', 'but', 'I', 'will', 'to', 'try', 'anothers', 'please', '?']\n",
      "2001\n"
     ]
    }
   ],
   "source": [
    "predictionTags = []\n",
    "\n",
    "def run_eval(sentences, gold_labels):\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for sents, labels in zip(sentences, gold_labels):\n",
    "        inputs = tokenizer(sents, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=True)\n",
    "        predictionTagOneSentence = []\n",
    "        with torch.no_grad():\n",
    "            word_ids = inputs.word_ids()\n",
    "            # tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].flatten())\n",
    "            logits = fine_tuned(**inputs).logits\n",
    "            predictions = torch.argmax(logits, dim=-1).flatten()\n",
    "            previous_word_idx = None\n",
    "            for idx, word_idx in enumerate(word_ids):\n",
    "                if previous_word_idx != word_idx and word_idx is not None:\n",
    "                    predictionTagOneSentence.append(label_vocab.idx2word[predictions[idx].item()])\n",
    "                previous_word_idx = word_idx\n",
    "        predictionTags.append(predictionTagOneSentence)\n",
    "\n",
    "        # Loop through gold labels\n",
    "        for goldLabel, predLabel in zip(labels, predictions):\n",
    "            if goldLabel.item() != 0:\n",
    "                total += 1\n",
    "                if goldLabel.item() == predLabel.item():\n",
    "                    match+= 1\n",
    "    return(match/total)\n",
    "    \n",
    "\n",
    "score = run_eval(dev_sents_tokenized[:len(dev_sents_tokenized)+1], dev_labels[:len(dev_labels)+1])\n",
    "\n",
    "print('Accuracy for dev data: {:.4f}'.format(score))\n",
    "\n",
    "print(dev_data[0][0])\n",
    "print(len(predictionTags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'new_bert_predictions_dev.iob2'), 'w') as f:\n",
    "    for sent_tokens, sent_preds in zip(dev_sents_tokenized[:len(dev_sents_tokenized)+1], predictionTags[:len(predictionTags)+1]):\n",
    "        for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# python3 span_f1.py data/new_bert_predictions_dev.iob2 data/en_ewt-ud-dev.iob2  <- run this in terminal to get span f1 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
