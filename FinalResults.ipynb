{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from checklist.perturb import Perturb\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "import os.path\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iob2_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "train_data= read_iob2_file('data//en_ewt-ud-train.iob2')\n",
    "dev_data = read_iob2_file('data//en_ewt-ud-dev.iob2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "PAD = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, pad_unk):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "\n",
    "max_len = max([len(x[0]) for x in train_data ])\n",
    "\n",
    "# Create vocabularies for both the tokens\n",
    "# and the tags\n",
    "token_vocab = Vocab(PAD)\n",
    "label_vocab = Vocab(PAD)\n",
    "id_to_token = [PAD]\n",
    "\n",
    "for tokens, tags in train_data:\n",
    "    for token in tokens:\n",
    "        token_vocab.getIdx(token, True)\n",
    "    for tag in tags:\n",
    "        label_vocab.getIdx(tag, True)\n",
    "\n",
    "NWORDS = len(token_vocab.idx2word)\n",
    "NTAGS = len(label_vocab.idx2word)\n",
    "\n",
    "# convert text data with labels to indices\n",
    "def data2feats(inputData, word_vocab, label_vocab):\n",
    "    feats = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    labels = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    for sentPos, sent in enumerate(inputData):\n",
    "        for wordPos, word in enumerate(sent[0][:max_len]):\n",
    "            wordIdx = word_vocab.getIdx(word)\n",
    "            feats[sentPos][wordPos] = wordIdx\n",
    "        for labelPos, label in enumerate(sent[1][:max_len]):\n",
    "            labelIdx = label_vocab.getIdx(label)\n",
    "            labels[sentPos][labelPos] = labelIdx\n",
    "    return feats, labels\n",
    "\n",
    "train_features, train_labels = data2feats(train_data, token_vocab, label_vocab)\n",
    "dev_feats, dev_labels = data2feats(dev_data, token_vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=NTAGS)\n",
    "def get_model():\n",
    "    return AutoModelForTokenClassification.from_pretrained(model_name, num_labels=NTAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  flat_labels = labels.flatten()\n",
    "  flat_preds = preds.flatten()\n",
    "  precision, recall, f1, _ = precision_recall_fscore_support(flat_labels, flat_preds, average='weighted')\n",
    "  \n",
    "  return {\n",
    "      'precision': precision,\n",
    "      'recall': recall,\n",
    "      'f1': f1,\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of training data = 12543\n",
    "Please edit the **number_of_sentences** to the desired number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-05, # was 2e-5 = 0.00002\n",
    "    per_device_train_batch_size=4, # was 16\n",
    "    num_train_epochs=5, # was 5\n",
    "    logging_steps=50, # was 250\n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.1,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=100,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sentences = 12543 # was 12543\n",
    "features = {'input_ids': train_features[:number_of_sentences], 'label': train_labels[:number_of_sentences]}\n",
    "train_dataset = Dataset.from_dict(features)\n",
    "dev_features = {'input_ids': dev_feats[:number_of_sentences], 'label': dev_labels[:number_of_sentences]}\n",
    "dev_dataset = Dataset.from_dict(dev_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part below needs to be run only **once**! - if we have the desired model saved in finetuned_bert, then you can skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ 50s / it for 100 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [5e-5, 3e-5, 2e-5]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2,3,4,5]),\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [0, 0.3]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    # model_init=get_model,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics, \n",
    ")\n",
    "#BestRun(run_id='0', objective=2.978584935884372, hyperparameters={'learning_rate': 2e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 4, 'weight_decay': 0}, run_summary=None) - newest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BestRun(run_id='2', objective=2.9824558048233256, hyperparameters={'learning_rate': 3e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 3, 'weight_decay': 0}, run_summary=None)\n",
    "# best_run = trainer.hyperparameter_search(\n",
    "#     direction=\"maximize\",\n",
    "#     backend=\"optuna\",\n",
    "#     hp_space=optuna_hp_space,\n",
    "#     n_trials=10,\n",
    "#     )\n",
    "    \n",
    "# best_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961d3d33755346ba93e0be52e53d2048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7798, 'learning_rate': 6.377551020408164e-07, 'epoch': 0.02}\n",
      "{'loss': 0.9568, 'learning_rate': 1.2755102040816329e-06, 'epoch': 0.03}\n",
      "{'loss': 0.3542, 'learning_rate': 1.913265306122449e-06, 'epoch': 0.05}\n",
      "{'loss': 0.1699, 'learning_rate': 2.5510204081632657e-06, 'epoch': 0.06}\n",
      "{'loss': 0.1047, 'learning_rate': 3.1887755102040818e-06, 'epoch': 0.08}\n",
      "{'loss': 0.0931, 'learning_rate': 3.826530612244898e-06, 'epoch': 0.1}\n",
      "{'loss': 0.082, 'learning_rate': 4.464285714285715e-06, 'epoch': 0.11}\n",
      "{'loss': 0.0621, 'learning_rate': 5.1020408163265315e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0545, 'learning_rate': 5.739795918367348e-06, 'epoch': 0.14}\n",
      "{'loss': 0.0343, 'learning_rate': 6.3775510204081635e-06, 'epoch': 0.16}\n",
      "{'loss': 0.0325, 'learning_rate': 7.01530612244898e-06, 'epoch': 0.18}\n",
      "{'loss': 0.0334, 'learning_rate': 7.653061224489796e-06, 'epoch': 0.19}\n",
      "{'loss': 0.0257, 'learning_rate': 8.290816326530612e-06, 'epoch': 0.21}\n",
      "{'loss': 0.0265, 'learning_rate': 8.92857142857143e-06, 'epoch': 0.22}\n",
      "{'loss': 0.0292, 'learning_rate': 9.566326530612246e-06, 'epoch': 0.24}\n",
      "{'loss': 0.0279, 'learning_rate': 1.0204081632653063e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0334, 'learning_rate': 1.0841836734693878e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0265, 'learning_rate': 1.1479591836734697e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0233, 'learning_rate': 1.2117346938775512e-05, 'epoch': 0.3}\n",
      "{'loss': 0.024, 'learning_rate': 1.2755102040816327e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0282, 'learning_rate': 1.3392857142857142e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0305, 'learning_rate': 1.403061224489796e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0224, 'learning_rate': 1.4668367346938776e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0344, 'learning_rate': 1.530612244897959e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0288, 'learning_rate': 1.594387755102041e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0251, 'learning_rate': 1.6581632653061225e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0267, 'learning_rate': 1.7219387755102043e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0269, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0258, 'learning_rate': 1.8494897959183674e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0273, 'learning_rate': 1.9132653061224492e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0221, 'learning_rate': 1.9770408163265307e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0282, 'learning_rate': 1.9954648526077098e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0301, 'learning_rate': 1.9883786848072564e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0217, 'learning_rate': 1.981292517006803e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0227, 'learning_rate': 1.9742063492063493e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0226, 'learning_rate': 1.967120181405896e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0178, 'learning_rate': 1.9600340136054422e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0228, 'learning_rate': 1.9529478458049888e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0326, 'learning_rate': 1.9458616780045354e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0202, 'learning_rate': 1.9387755102040817e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0246, 'learning_rate': 1.9316893424036283e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0239, 'learning_rate': 1.924603174603175e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0242, 'learning_rate': 1.9175170068027212e-05, 'epoch': 0.69}\n",
      "{'loss': 0.018, 'learning_rate': 1.9104308390022678e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0277, 'learning_rate': 1.903344671201814e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0228, 'learning_rate': 1.8962585034013607e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0236, 'learning_rate': 1.8891723356009073e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0195, 'learning_rate': 1.8820861678004536e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0201, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0211, 'learning_rate': 1.8679138321995468e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0227, 'learning_rate': 1.860827664399093e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0166, 'learning_rate': 1.8537414965986397e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0213, 'learning_rate': 1.846655328798186e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0214, 'learning_rate': 1.8395691609977326e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0196, 'learning_rate': 1.8324829931972788e-05, 'epoch': 0.88}\n",
      "{'loss': 0.0232, 'learning_rate': 1.8253968253968254e-05, 'epoch': 0.89}\n",
      "{'loss': 0.023, 'learning_rate': 1.818310657596372e-05, 'epoch': 0.91}\n",
      "{'loss': 0.019, 'learning_rate': 1.8112244897959187e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0209, 'learning_rate': 1.804138321995465e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0195, 'learning_rate': 1.7970521541950115e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0177, 'learning_rate': 1.789965986394558e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0223, 'learning_rate': 1.7828798185941044e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0181, 'learning_rate': 1.7757936507936507e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0139, 'learning_rate': 1.7687074829931973e-05, 'epoch': 1.02}\n",
      "{'loss': 0.018, 'learning_rate': 1.761621315192744e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0211, 'learning_rate': 1.7545351473922905e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0186, 'learning_rate': 1.7474489795918368e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0208, 'learning_rate': 1.7403628117913834e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0139, 'learning_rate': 1.73327664399093e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0197, 'learning_rate': 1.7261904761904763e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0184, 'learning_rate': 1.7191043083900226e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0173, 'learning_rate': 1.7120181405895692e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0171, 'learning_rate': 1.7049319727891158e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0138, 'learning_rate': 1.6978458049886624e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0164, 'learning_rate': 1.6907596371882087e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0145, 'learning_rate': 1.6836734693877553e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0167, 'learning_rate': 1.676587301587302e-05, 'epoch': 1.23}\n",
      "{'loss': 0.019, 'learning_rate': 1.6695011337868482e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0184, 'learning_rate': 1.6624149659863948e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0171, 'learning_rate': 1.655328798185941e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0169, 'learning_rate': 1.6482426303854877e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0187, 'learning_rate': 1.6411564625850343e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0156, 'learning_rate': 1.6340702947845805e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0132, 'learning_rate': 1.626984126984127e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0162, 'learning_rate': 1.6198979591836738e-05, 'epoch': 1.36}\n",
      "{'loss': 0.014, 'learning_rate': 1.61281179138322e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0195, 'learning_rate': 1.6057256235827667e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0162, 'learning_rate': 1.598639455782313e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0114, 'learning_rate': 1.5915532879818595e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0165, 'learning_rate': 1.5844671201814058e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0158, 'learning_rate': 1.5773809523809524e-05, 'epoch': 1.45}\n",
      "{'loss': 0.014, 'learning_rate': 1.570294784580499e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0115, 'learning_rate': 1.5632086167800456e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0166, 'learning_rate': 1.556122448979592e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0164, 'learning_rate': 1.5490362811791385e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0149, 'learning_rate': 1.5419501133786848e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0145, 'learning_rate': 1.5348639455782314e-05, 'epoch': 1.55}\n",
      "{'loss': 0.013, 'learning_rate': 1.5277777777777777e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0143, 'learning_rate': 1.5206916099773243e-05, 'epoch': 1.58}\n",
      "{'loss': 0.015, 'learning_rate': 1.5136054421768709e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0144, 'learning_rate': 1.5065192743764173e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0154, 'learning_rate': 1.4994331065759638e-05, 'epoch': 1.63}\n",
      "{'loss': 0.014, 'learning_rate': 1.4923469387755102e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0155, 'learning_rate': 1.4852607709750568e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0139, 'learning_rate': 1.4781746031746035e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0123, 'learning_rate': 1.4710884353741497e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0171, 'learning_rate': 1.4640022675736962e-05, 'epoch': 1.71}\n",
      "{'loss': 0.013, 'learning_rate': 1.4569160997732428e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0104, 'learning_rate': 1.4498299319727894e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0102, 'learning_rate': 1.4427437641723357e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0145, 'learning_rate': 1.4356575963718821e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0075, 'learning_rate': 1.4285714285714287e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0124, 'learning_rate': 1.4214852607709753e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0132, 'learning_rate': 1.4143990929705216e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0111, 'learning_rate': 1.4073129251700682e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0079, 'learning_rate': 1.4002267573696146e-05, 'epoch': 1.85}\n",
      "{'loss': 0.011, 'learning_rate': 1.3931405895691613e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0117, 'learning_rate': 1.3860544217687075e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0146, 'learning_rate': 1.3789682539682541e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0097, 'learning_rate': 1.3718820861678006e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0131, 'learning_rate': 1.3647959183673472e-05, 'epoch': 1.93}\n",
      "{'loss': 0.0124, 'learning_rate': 1.3577097505668935e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0112, 'learning_rate': 1.35062358276644e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0128, 'learning_rate': 1.3435374149659865e-05, 'epoch': 1.98}\n",
      "{'loss': 0.0115, 'learning_rate': 1.3364512471655331e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0103, 'learning_rate': 1.3293650793650794e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0113, 'learning_rate': 1.322278911564626e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0107, 'learning_rate': 1.3151927437641725e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0093, 'learning_rate': 1.308106575963719e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0108, 'learning_rate': 1.3010204081632653e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0072, 'learning_rate': 1.293934240362812e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0077, 'learning_rate': 1.2868480725623584e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0072, 'learning_rate': 1.2797619047619048e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0115, 'learning_rate': 1.2726757369614513e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0097, 'learning_rate': 1.2655895691609979e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0094, 'learning_rate': 1.2585034013605443e-05, 'epoch': 2.17}\n",
      "{'loss': 0.0099, 'learning_rate': 1.2514172335600908e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0088, 'learning_rate': 1.2443310657596372e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0097, 'learning_rate': 1.2372448979591838e-05, 'epoch': 2.22}\n",
      "{'loss': 0.0078, 'learning_rate': 1.2301587301587303e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0112, 'learning_rate': 1.2230725623582767e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0096, 'learning_rate': 1.2159863945578231e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0095, 'learning_rate': 1.2089002267573698e-05, 'epoch': 2.28}\n",
      "{'loss': 0.0069, 'learning_rate': 1.2018140589569162e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0068, 'learning_rate': 1.1947278911564626e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0111, 'learning_rate': 1.1876417233560091e-05, 'epoch': 2.33}\n",
      "{'loss': 0.0103, 'learning_rate': 1.1805555555555557e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0073, 'learning_rate': 1.1734693877551021e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0089, 'learning_rate': 1.1663832199546486e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0088, 'learning_rate': 1.159297052154195e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0085, 'learning_rate': 1.1522108843537416e-05, 'epoch': 2.41}\n",
      "{'loss': 0.0083, 'learning_rate': 1.1451247165532882e-05, 'epoch': 2.42}\n",
      "{'loss': 0.008, 'learning_rate': 1.1380385487528345e-05, 'epoch': 2.44}\n",
      "{'loss': 0.0076, 'learning_rate': 1.130952380952381e-05, 'epoch': 2.46}\n",
      "{'loss': 0.0086, 'learning_rate': 1.1238662131519276e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0109, 'learning_rate': 1.1167800453514742e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0078, 'learning_rate': 1.1096938775510205e-05, 'epoch': 2.5}\n",
      "{'loss': 0.007, 'learning_rate': 1.1026077097505669e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0086, 'learning_rate': 1.0955215419501135e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0073, 'learning_rate': 1.0884353741496601e-05, 'epoch': 2.55}\n",
      "{'loss': 0.0089, 'learning_rate': 1.0813492063492064e-05, 'epoch': 2.57}\n",
      "{'loss': 0.0111, 'learning_rate': 1.0742630385487528e-05, 'epoch': 2.58}\n",
      "{'loss': 0.0065, 'learning_rate': 1.0671768707482994e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0069, 'learning_rate': 1.060090702947846e-05, 'epoch': 2.61}\n",
      "{'loss': 0.006, 'learning_rate': 1.0530045351473923e-05, 'epoch': 2.63}\n",
      "{'loss': 0.0085, 'learning_rate': 1.045918367346939e-05, 'epoch': 2.65}\n",
      "{'loss': 0.0069, 'learning_rate': 1.0388321995464854e-05, 'epoch': 2.66}\n",
      "{'loss': 0.0077, 'learning_rate': 1.031746031746032e-05, 'epoch': 2.68}\n",
      "{'loss': 0.007, 'learning_rate': 1.0246598639455783e-05, 'epoch': 2.69}\n",
      "{'loss': 0.0105, 'learning_rate': 1.0175736961451249e-05, 'epoch': 2.71}\n",
      "{'loss': 0.0081, 'learning_rate': 1.0104875283446713e-05, 'epoch': 2.73}\n",
      "{'loss': 0.0091, 'learning_rate': 1.0034013605442176e-05, 'epoch': 2.74}\n",
      "{'loss': 0.0083, 'learning_rate': 9.963151927437642e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0093, 'learning_rate': 9.892290249433108e-06, 'epoch': 2.77}\n",
      "{'loss': 0.009, 'learning_rate': 9.821428571428573e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0094, 'learning_rate': 9.750566893424037e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0077, 'learning_rate': 9.679705215419501e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0054, 'learning_rate': 9.608843537414967e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0067, 'learning_rate': 9.537981859410432e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0053, 'learning_rate': 9.467120181405896e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0064, 'learning_rate': 9.39625850340136e-06, 'epoch': 2.89}\n",
      "{'loss': 0.0098, 'learning_rate': 9.325396825396827e-06, 'epoch': 2.9}\n",
      "{'loss': 0.007, 'learning_rate': 9.254535147392291e-06, 'epoch': 2.92}\n",
      "{'loss': 0.007, 'learning_rate': 9.183673469387756e-06, 'epoch': 2.93}\n",
      "{'loss': 0.007, 'learning_rate': 9.11281179138322e-06, 'epoch': 2.95}\n",
      "{'loss': 0.0076, 'learning_rate': 9.041950113378686e-06, 'epoch': 2.97}\n",
      "{'loss': 0.0081, 'learning_rate': 8.97108843537415e-06, 'epoch': 2.98}\n",
      "{'loss': 0.0077, 'learning_rate': 8.900226757369615e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0074, 'learning_rate': 8.82936507936508e-06, 'epoch': 3.01}\n",
      "{'loss': 0.0061, 'learning_rate': 8.758503401360546e-06, 'epoch': 3.03}\n",
      "{'loss': 0.0067, 'learning_rate': 8.68764172335601e-06, 'epoch': 3.05}\n",
      "{'loss': 0.0064, 'learning_rate': 8.616780045351474e-06, 'epoch': 3.06}\n",
      "{'loss': 0.0047, 'learning_rate': 8.545918367346939e-06, 'epoch': 3.08}\n",
      "{'loss': 0.0038, 'learning_rate': 8.475056689342405e-06, 'epoch': 3.09}\n",
      "{'loss': 0.0043, 'learning_rate': 8.40419501133787e-06, 'epoch': 3.11}\n",
      "{'loss': 0.0055, 'learning_rate': 8.333333333333334e-06, 'epoch': 3.12}\n",
      "{'loss': 0.005, 'learning_rate': 8.262471655328798e-06, 'epoch': 3.14}\n",
      "{'loss': 0.0049, 'learning_rate': 8.191609977324264e-06, 'epoch': 3.16}\n",
      "{'loss': 0.0049, 'learning_rate': 8.120748299319729e-06, 'epoch': 3.17}\n",
      "{'loss': 0.0068, 'learning_rate': 8.049886621315193e-06, 'epoch': 3.19}\n",
      "{'loss': 0.0053, 'learning_rate': 7.979024943310658e-06, 'epoch': 3.2}\n",
      "{'loss': 0.0061, 'learning_rate': 7.908163265306124e-06, 'epoch': 3.22}\n",
      "{'loss': 0.0095, 'learning_rate': 7.837301587301588e-06, 'epoch': 3.24}\n",
      "{'loss': 0.0053, 'learning_rate': 7.766439909297052e-06, 'epoch': 3.25}\n",
      "{'loss': 0.005, 'learning_rate': 7.695578231292517e-06, 'epoch': 3.27}\n",
      "{'loss': 0.0058, 'learning_rate': 7.624716553287983e-06, 'epoch': 3.28}\n",
      "{'loss': 0.0044, 'learning_rate': 7.553854875283447e-06, 'epoch': 3.3}\n",
      "{'loss': 0.0077, 'learning_rate': 7.482993197278913e-06, 'epoch': 3.32}\n",
      "{'loss': 0.006, 'learning_rate': 7.412131519274377e-06, 'epoch': 3.33}\n",
      "{'loss': 0.0045, 'learning_rate': 7.3412698412698415e-06, 'epoch': 3.35}\n",
      "{'loss': 0.0063, 'learning_rate': 7.270408163265307e-06, 'epoch': 3.36}\n",
      "{'loss': 0.0041, 'learning_rate': 7.199546485260771e-06, 'epoch': 3.38}\n",
      "{'loss': 0.0077, 'learning_rate': 7.1286848072562364e-06, 'epoch': 3.4}\n",
      "{'loss': 0.0053, 'learning_rate': 7.057823129251701e-06, 'epoch': 3.41}\n",
      "{'loss': 0.0068, 'learning_rate': 6.986961451247166e-06, 'epoch': 3.43}\n",
      "{'loss': 0.0044, 'learning_rate': 6.9160997732426305e-06, 'epoch': 3.44}\n",
      "{'loss': 0.0055, 'learning_rate': 6.845238095238096e-06, 'epoch': 3.46}\n",
      "{'loss': 0.0045, 'learning_rate': 6.77437641723356e-06, 'epoch': 3.48}\n",
      "{'loss': 0.0077, 'learning_rate': 6.7035147392290255e-06, 'epoch': 3.49}\n",
      "{'loss': 0.0063, 'learning_rate': 6.63265306122449e-06, 'epoch': 3.51}\n",
      "{'loss': 0.0072, 'learning_rate': 6.561791383219955e-06, 'epoch': 3.52}\n",
      "{'loss': 0.0061, 'learning_rate': 6.49092970521542e-06, 'epoch': 3.54}\n",
      "{'loss': 0.0038, 'learning_rate': 6.420068027210885e-06, 'epoch': 3.56}\n",
      "{'loss': 0.0055, 'learning_rate': 6.349206349206349e-06, 'epoch': 3.57}\n",
      "{'loss': 0.0073, 'learning_rate': 6.2783446712018145e-06, 'epoch': 3.59}\n",
      "{'loss': 0.0046, 'learning_rate': 6.207482993197279e-06, 'epoch': 3.6}\n",
      "{'loss': 0.0046, 'learning_rate': 6.136621315192744e-06, 'epoch': 3.62}\n",
      "{'loss': 0.0054, 'learning_rate': 6.065759637188209e-06, 'epoch': 3.64}\n",
      "{'loss': 0.0062, 'learning_rate': 5.994897959183674e-06, 'epoch': 3.65}\n",
      "{'loss': 0.0046, 'learning_rate': 5.924036281179138e-06, 'epoch': 3.67}\n",
      "{'loss': 0.0047, 'learning_rate': 5.8531746031746036e-06, 'epoch': 3.68}\n",
      "{'loss': 0.0049, 'learning_rate': 5.782312925170068e-06, 'epoch': 3.7}\n",
      "{'loss': 0.0065, 'learning_rate': 5.711451247165533e-06, 'epoch': 3.71}\n",
      "{'loss': 0.0054, 'learning_rate': 5.640589569160998e-06, 'epoch': 3.73}\n",
      "{'loss': 0.0066, 'learning_rate': 5.569727891156464e-06, 'epoch': 3.75}\n",
      "{'loss': 0.0049, 'learning_rate': 5.498866213151927e-06, 'epoch': 3.76}\n",
      "{'loss': 0.0045, 'learning_rate': 5.4280045351473935e-06, 'epoch': 3.78}\n",
      "{'loss': 0.006, 'learning_rate': 5.357142857142857e-06, 'epoch': 3.79}\n",
      "{'loss': 0.0049, 'learning_rate': 5.286281179138323e-06, 'epoch': 3.81}\n",
      "{'loss': 0.0048, 'learning_rate': 5.2154195011337876e-06, 'epoch': 3.83}\n",
      "{'loss': 0.0044, 'learning_rate': 5.144557823129253e-06, 'epoch': 3.84}\n",
      "{'loss': 0.0033, 'learning_rate': 5.073696145124717e-06, 'epoch': 3.86}\n",
      "{'loss': 0.0049, 'learning_rate': 5.0028344671201825e-06, 'epoch': 3.87}\n",
      "{'loss': 0.0044, 'learning_rate': 4.931972789115647e-06, 'epoch': 3.89}\n",
      "{'loss': 0.0066, 'learning_rate': 4.861111111111111e-06, 'epoch': 3.91}\n",
      "{'loss': 0.0041, 'learning_rate': 4.790249433106577e-06, 'epoch': 3.92}\n",
      "{'loss': 0.0048, 'learning_rate': 4.719387755102041e-06, 'epoch': 3.94}\n",
      "{'loss': 0.0054, 'learning_rate': 4.648526077097506e-06, 'epoch': 3.95}\n",
      "{'loss': 0.0036, 'learning_rate': 4.577664399092971e-06, 'epoch': 3.97}\n",
      "{'loss': 0.0043, 'learning_rate': 4.506802721088436e-06, 'epoch': 3.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0041, 'learning_rate': 4.4359410430839e-06, 'epoch': 4.0}\n",
      "{'loss': 0.0036, 'learning_rate': 4.365079365079366e-06, 'epoch': 4.02}\n",
      "{'loss': 0.0042, 'learning_rate': 4.29421768707483e-06, 'epoch': 4.03}\n",
      "{'loss': 0.0033, 'learning_rate': 4.223356009070295e-06, 'epoch': 4.05}\n",
      "{'loss': 0.0036, 'learning_rate': 4.15249433106576e-06, 'epoch': 4.07}\n",
      "{'loss': 0.0028, 'learning_rate': 4.081632653061225e-06, 'epoch': 4.08}\n",
      "{'loss': 0.0038, 'learning_rate': 4.0107709750566894e-06, 'epoch': 4.1}\n",
      "{'loss': 0.0043, 'learning_rate': 3.939909297052155e-06, 'epoch': 4.11}\n",
      "{'loss': 0.0048, 'learning_rate': 3.869047619047619e-06, 'epoch': 4.13}\n",
      "{'loss': 0.0033, 'learning_rate': 3.7981859410430844e-06, 'epoch': 4.15}\n",
      "{'loss': 0.0026, 'learning_rate': 3.7273242630385492e-06, 'epoch': 4.16}\n",
      "{'loss': 0.0041, 'learning_rate': 3.656462585034014e-06, 'epoch': 4.18}\n",
      "{'loss': 0.0026, 'learning_rate': 3.585600907029479e-06, 'epoch': 4.19}\n",
      "{'loss': 0.0042, 'learning_rate': 3.5147392290249437e-06, 'epoch': 4.21}\n",
      "{'loss': 0.0037, 'learning_rate': 3.4438775510204086e-06, 'epoch': 4.23}\n",
      "{'loss': 0.0034, 'learning_rate': 3.3730158730158734e-06, 'epoch': 4.24}\n",
      "{'loss': 0.0054, 'learning_rate': 3.3021541950113383e-06, 'epoch': 4.26}\n",
      "{'loss': 0.0046, 'learning_rate': 3.231292517006803e-06, 'epoch': 4.27}\n",
      "{'loss': 0.0043, 'learning_rate': 3.160430839002268e-06, 'epoch': 4.29}\n",
      "{'loss': 0.0048, 'learning_rate': 3.0895691609977324e-06, 'epoch': 4.3}\n",
      "{'loss': 0.0047, 'learning_rate': 3.018707482993197e-06, 'epoch': 4.32}\n",
      "{'loss': 0.0033, 'learning_rate': 2.947845804988662e-06, 'epoch': 4.34}\n",
      "{'loss': 0.005, 'learning_rate': 2.876984126984127e-06, 'epoch': 4.35}\n",
      "{'loss': 0.0045, 'learning_rate': 2.8061224489795917e-06, 'epoch': 4.37}\n",
      "{'loss': 0.0041, 'learning_rate': 2.7352607709750566e-06, 'epoch': 4.38}\n",
      "{'loss': 0.0041, 'learning_rate': 2.6643990929705214e-06, 'epoch': 4.4}\n",
      "{'loss': 0.0037, 'learning_rate': 2.5935374149659862e-06, 'epoch': 4.42}\n",
      "{'loss': 0.004, 'learning_rate': 2.522675736961451e-06, 'epoch': 4.43}\n",
      "{'loss': 0.0031, 'learning_rate': 2.4518140589569163e-06, 'epoch': 4.45}\n",
      "{'loss': 0.0026, 'learning_rate': 2.380952380952381e-06, 'epoch': 4.46}\n",
      "{'loss': 0.0045, 'learning_rate': 2.310090702947846e-06, 'epoch': 4.48}\n",
      "{'loss': 0.0044, 'learning_rate': 2.239229024943311e-06, 'epoch': 4.5}\n",
      "{'loss': 0.0031, 'learning_rate': 2.1683673469387757e-06, 'epoch': 4.51}\n",
      "{'loss': 0.0038, 'learning_rate': 2.0975056689342405e-06, 'epoch': 4.53}\n",
      "{'loss': 0.0032, 'learning_rate': 2.0266439909297054e-06, 'epoch': 4.54}\n",
      "{'loss': 0.0032, 'learning_rate': 1.9557823129251702e-06, 'epoch': 4.56}\n",
      "{'loss': 0.0037, 'learning_rate': 1.884920634920635e-06, 'epoch': 4.58}\n",
      "{'loss': 0.0032, 'learning_rate': 1.8140589569161e-06, 'epoch': 4.59}\n",
      "{'loss': 0.0024, 'learning_rate': 1.7431972789115648e-06, 'epoch': 4.61}\n",
      "{'loss': 0.0043, 'learning_rate': 1.6723356009070296e-06, 'epoch': 4.62}\n",
      "{'loss': 0.0048, 'learning_rate': 1.6014739229024944e-06, 'epoch': 4.64}\n",
      "{'loss': 0.0029, 'learning_rate': 1.5306122448979593e-06, 'epoch': 4.66}\n",
      "{'loss': 0.0037, 'learning_rate': 1.4597505668934241e-06, 'epoch': 4.67}\n",
      "{'loss': 0.0045, 'learning_rate': 1.3888888888888892e-06, 'epoch': 4.69}\n",
      "{'loss': 0.0037, 'learning_rate': 1.318027210884354e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0027, 'learning_rate': 1.2471655328798186e-06, 'epoch': 4.72}\n",
      "{'loss': 0.003, 'learning_rate': 1.1763038548752835e-06, 'epoch': 4.74}\n",
      "{'loss': 0.0032, 'learning_rate': 1.1054421768707483e-06, 'epoch': 4.75}\n",
      "{'loss': 0.0022, 'learning_rate': 1.0345804988662132e-06, 'epoch': 4.77}\n",
      "{'loss': 0.002, 'learning_rate': 9.63718820861678e-07, 'epoch': 4.78}\n",
      "{'loss': 0.0037, 'learning_rate': 8.928571428571429e-07, 'epoch': 4.8}\n",
      "{'loss': 0.0021, 'learning_rate': 8.219954648526078e-07, 'epoch': 4.82}\n",
      "{'loss': 0.0055, 'learning_rate': 7.511337868480726e-07, 'epoch': 4.83}\n",
      "{'loss': 0.0025, 'learning_rate': 6.802721088435376e-07, 'epoch': 4.85}\n",
      "{'loss': 0.0042, 'learning_rate': 6.094104308390023e-07, 'epoch': 4.86}\n",
      "{'loss': 0.0032, 'learning_rate': 5.385487528344671e-07, 'epoch': 4.88}\n",
      "{'loss': 0.005, 'learning_rate': 4.6768707482993204e-07, 'epoch': 4.89}\n",
      "{'loss': 0.0036, 'learning_rate': 3.9682539682539683e-07, 'epoch': 4.91}\n",
      "{'loss': 0.0035, 'learning_rate': 3.259637188208617e-07, 'epoch': 4.93}\n",
      "{'loss': 0.0019, 'learning_rate': 2.5510204081632656e-07, 'epoch': 4.94}\n",
      "{'loss': 0.0023, 'learning_rate': 1.8424036281179138e-07, 'epoch': 4.96}\n",
      "{'loss': 0.0055, 'learning_rate': 1.1337868480725624e-07, 'epoch': 4.97}\n",
      "{'loss': 0.0027, 'learning_rate': 4.25170068027211e-08, 'epoch': 4.99}\n",
      "{'train_runtime': 5105.1273, 'train_samples_per_second': 12.285, 'train_steps_per_second': 3.071, 'train_loss': 0.022309727647474836, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# for n, v in best_run.hyperparameters.items():\n",
    "#     setattr(trainer.args, n, v)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"distil_bert_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model overview: \n",
      "LangID(\n",
      "  (word_embeddings): Embedding(19674, 100)\n",
      "  (bilstm): LSTM(100, 50, batch_first=True)\n",
      "  (hidden_to_tag): Linear(in_features=50, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LangID(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_dim, vocab_dim):\n",
    "        super(LangID, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_dim, embed_dim)\n",
    "        self.bilstm = nn.LSTM(embed_dim, lstm_dim, bidirectional=False, batch_first=True)\n",
    "        self.hidden_to_tag = nn.Linear(lstm_dim, NTAGS)\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        word_vectors = self.word_embeddings(inputs)\n",
    "        bilstm_out, _ = self.bilstm(word_vectors)\n",
    "        y = self.hidden_to_tag(bilstm_out)\n",
    "        return y # softmax this in order to get probs, check out for axis, has to sum up to 1\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            data_feats, data_labels = data2feats(inputs, token_vocab, label_vocab)\n",
    "\n",
    "            logits = self.forward(data_feats)\n",
    "            probabilities = self.softmax(logits)\n",
    "            return torch.argmax(probabilities, 2)\n",
    "\n",
    "\n",
    "# define the model\n",
    "langid_model = LangID(DIM_EMBEDDING, LSTM_HIDDEN, NWORDS)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "optimizer = optim.Adam(langid_model.parameters(), lr=LEARNING_RATE)\n",
    "print('model overview: ')\n",
    "print(langid_model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to batches\n",
    "num_batches = int(len(train_features)/BATCH_SIZE)\n",
    "train_feats_batches = train_features[:BATCH_SIZE*num_batches].view(num_batches, BATCH_SIZE, max_len)\n",
    "train_labels_batches = train_labels[:BATCH_SIZE*num_batches].view(num_batches, BATCH_SIZE, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   loss      Train acc.\n",
      "0       271.02    0.9431\n",
      "1       113.19    0.9683\n",
      "2       59.08     0.9827\n",
      "3       35.58     0.9899\n",
      "4       23.35     0.9935\n"
     ]
    }
   ],
   "source": [
    "print('epoch   loss      Train acc.')\n",
    "for epoch in range(EPOCHS):\n",
    "    langid_model.train() \n",
    "    langid_model.zero_grad()\n",
    "\n",
    "    # Loop over batches\n",
    "    loss = 0\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for batchIdx in range(0, num_batches):\n",
    "        output_scores = langid_model.forward(train_feats_batches[batchIdx])\n",
    "        \n",
    "        output_scores = output_scores.view(BATCH_SIZE * max_len, -1)\n",
    "        flat_labels = train_labels_batches[batchIdx].view(BATCH_SIZE * max_len)\n",
    "        batch_loss = loss_function(output_scores, flat_labels)\n",
    "\n",
    "        predicted_labels = torch.argmax(output_scores, 1)\n",
    "        predicted_labels = predicted_labels.view(BATCH_SIZE, max_len)\n",
    "\n",
    "        # Run backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        langid_model.zero_grad()\n",
    "        loss += batch_loss.item()\n",
    "        # Update the number of correct tags and total tags\n",
    "        for gold_sent, pred_sent in zip(train_labels_batches[batchIdx], predicted_labels):\n",
    "            for gold_label, pred_label in zip(gold_sent, pred_sent):\n",
    "                if gold_label != 0:\n",
    "                    total += 1\n",
    "                    if gold_label == pred_label:\n",
    "                        match+= 1\n",
    "    print('{0: <8}{1: <10}{2}'.format(epoch, '{:.2f}'.format(loss/num_batches), '{:.4f}'.format(match / total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load fine tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our finetuned model\n",
    "fine_tuned = AutoModelForTokenClassification.from_pretrained(\"distil_bert_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForTokenClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(fine_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our model on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dev data: 0.9278\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "def run_eval(feats_batches, labels_batches, model):\n",
    "    if model == 'LSTM':\n",
    "        langid_model.eval()\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for sents, labels in zip(feats_batches, labels_batches):\n",
    "        if model == 'LSTM':\n",
    "            output_scores = langid_model.forward(sents)\n",
    "            predicted_tags  = torch.argmax(output_scores, 2)\n",
    "        elif model == 'BERT':\n",
    "            output_scores = fine_tuned(sents) \n",
    "            predicted_tags  = torch.argmax(output_scores.logits, dim=-1)\n",
    "        else:\n",
    "            print('Please specify supported model.')\n",
    "            return\n",
    "        for sentence in sents:\n",
    "            sentenceWords = []\n",
    "            for wordIndex in sentence:\n",
    "                sentenceWords.append(token_vocab.getWord(wordIndex.item()))\n",
    "            sentences.append(sentenceWords)\n",
    "        for sentenceTags in predicted_tags:\n",
    "                predictionTagOneSentence = []\n",
    "                for tag in sentenceTags:\n",
    "                    predictionTagOneSentence.append(label_vocab.idx2word[tag.item()])\n",
    "                predictions.append(predictionTagOneSentence)\n",
    "        for goldSent, predSent in zip(labels, predicted_tags):\n",
    "            for goldLabel, predLabel in zip(goldSent, predSent):\n",
    "                if goldLabel.item() != 0:\n",
    "                    total += 1\n",
    "                    if goldLabel.item() == predLabel.item():\n",
    "                        match+= 1\n",
    "    return(match/total)\n",
    "\n",
    "num_batches_dev = int(len(dev_feats)/BATCH_SIZE)\n",
    "\n",
    "dev_feats_batches = dev_feats[:BATCH_SIZE*num_batches_dev].view(num_batches_dev, BATCH_SIZE, max_len)\n",
    "dev_labels_batches = dev_labels[:BATCH_SIZE*num_batches_dev].view(num_batches_dev, BATCH_SIZE, max_len)\n",
    "score = run_eval(dev_feats_batches, dev_labels_batches, 'BERT')\n",
    "\n",
    "print('Accuracy for dev data: {:.4f}'.format(score))\n",
    "\n",
    "# with open(os.path.join('data', 'bert_predictions_dev.iob2'), 'w') as f:\n",
    "#     for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "#         for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "#             f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# python3 span_f1.py data/bert_predictions_dev.iob2 data/en_ewt-ud-dev.iob2  <- run this in terminal to get span f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "dataset = []\n",
    "for sentence in dev_data:\n",
    "    dataset.append(\" \".join(sentence[0]))\n",
    "pdataset = list(nlp.pipe(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []\n",
    "new_tagged_dataset = []\n",
    "# nsamples = how many 'name' sentences we want to take into account\n",
    "# n = represents number of sentences that we want to generate for each 'name' sentence\n",
    "t_names = Perturb.perturb(pdataset, Perturb.change_names, n=2)\n",
    "original_sentences = []\n",
    "for sentences in t_names.data:\n",
    "    original_sentences.append(sentences[0])\n",
    "# Tokenize\n",
    "for sentences in t_names.data:\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append(sentence.split())\n",
    "# Assign NER tags to the generated data\n",
    "for index, new_sentence in enumerate(new_sentences):\n",
    "    for sentence in dev_data:\n",
    "        if new_sentence == sentence[0]:\n",
    "            ner_tag = sentence[1]\n",
    "    if index % 3 != 0:\n",
    "        new_tagged_dataset.append((new_sentences[index],ner_tag))\n",
    "\n",
    "# Create gold labels file: index<TAB>word<TAB>label. \n",
    "# with open(os.path.join('data', 'gold_names.iob2'), 'w') as f:\n",
    "#     for sentence, tag in new_tagged_dataset:\n",
    "#         for index, (token, pred) in enumerate(zip(sentence, tag)):\n",
    "#             f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "changed_names_feats, dev_names_labels = data2feats(new_tagged_dataset, token_vocab, label_vocab)\n",
    "num_batches_changed_names = int(len(changed_names_feats)/BATCH_SIZE)\n",
    "\n",
    "changed_names_feats_batches = changed_names_feats[:BATCH_SIZE*num_batches_changed_names].view(num_batches_changed_names, BATCH_SIZE, max_len)\n",
    "changed_names_labels_batches = dev_names_labels[:BATCH_SIZE*num_batches_changed_names].view(num_batches_changed_names, BATCH_SIZE, max_len)\n",
    "# score = run_eval(changed_names_feats_batches, changed_names_labels_batches, 'BERT')\n",
    "\n",
    "# print('\\033[32mAccuracy for changed names data: \\033[0m {:.4f}'.format(score))\n",
    "\n",
    "# with open(os.path.join('data', 'bert_predictions_names.iob2'), 'w') as f:\n",
    "#     for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "#         for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "#             f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "    \n",
    "# python3 span_f1.py data/bert_predictions_names.iob2 data/gold_names.iob2 <- run this in terminal to get span f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []\n",
    "new_tagged_dataset = []\n",
    "# nsamples = how many 'name' sentences we want to take into account\n",
    "# n = represents number of sentences that we want to generate for each 'name' sentence\n",
    "t_location = Perturb.perturb(pdataset, Perturb.change_location, n=2)\n",
    "original_sentences = []\n",
    "for sentences in t_location.data:\n",
    "    original_sentences.append(sentences[0])\n",
    "# Tokenize\n",
    "for sentences in t_location.data:\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append(sentence.split())\n",
    "# Assign NER tags to the generated data\n",
    "for index, new_sentence in enumerate(new_sentences):\n",
    "    for sentence in dev_data:\n",
    "        if new_sentence == sentence[0]:\n",
    "            ner_tag = sentence[1]\n",
    "    if index % 3 != 0:\n",
    "        new_tagged_dataset.append((new_sentences[index],ner_tag))\n",
    "\n",
    "# Create gold labels file: index<TAB>word<TAB>label. \n",
    "# with open(os.path.join('data', 'gold_location.iob2'), 'w') as f:\n",
    "#     for sentence, tag in new_tagged_dataset:\n",
    "#         for index, (token, pred) in enumerate(zip(sentence, tag)):\n",
    "#             f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "changed_location_feats, dev_location_labels = data2feats(new_tagged_dataset, token_vocab, label_vocab)\n",
    "num_batches_changed_location = int(len(changed_location_feats)/BATCH_SIZE)\n",
    "\n",
    "changed_location_feats_batches = changed_location_feats[:BATCH_SIZE*num_batches_changed_location].view(num_batches_changed_location, BATCH_SIZE, max_len)\n",
    "changed_location_labels_batches = dev_location_labels[:BATCH_SIZE*num_batches_changed_location].view(num_batches_changed_location, BATCH_SIZE, max_len)\n",
    "# score = run_eval(changed_location_feats_batches, changed_location_labels_batches, 'BERT')\n",
    "\n",
    "# print('\\033[32mAccuracy for changed location data: \\033[0m {:.4f}'.format(score))\n",
    "\n",
    "# with open(os.path.join('data', 'bert_predictions_location.iob2'), 'w') as f:\n",
    "#     for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "#         for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "#             f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# python3 span_f1.py data/bert_predictions_location.iob2 data/gold_location.iob2 <- run this in terminal to get span f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []\n",
    "new_tagged_dataset = []\n",
    "# nsamples = how many 'name' sentences we want to take into account\n",
    "# n = represents number of sentences that we want to generate for each 'name' sentence\n",
    "t_number = Perturb.perturb(pdataset, Perturb.change_number, n=2)\n",
    "original_sentences = []\n",
    "for sentences in t_number.data:\n",
    "    original_sentences.append(sentences[0])\n",
    "# Tokenize\n",
    "for sentences in t_number.data:\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append(sentence.split())\n",
    "# Assign NER tags to the generated data\n",
    "for index, new_sentence in enumerate(new_sentences):\n",
    "    for sentence in dev_data:\n",
    "        if new_sentence == sentence[0]:\n",
    "            ner_tag = sentence[1]\n",
    "    if index % 3 != 0:\n",
    "        new_tagged_dataset.append((new_sentences[index],ner_tag))\n",
    "\n",
    "# Create gold labels file: index<TAB>word<TAB>label. \n",
    "# with open(os.path.join('data', 'gold_numbers.iob2'), 'w') as f:\n",
    "#     for sentence, tag in new_tagged_dataset:\n",
    "#         for index, (token, pred) in enumerate(zip(sentence, tag)):\n",
    "#             f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "changed_number_feats, dev_numbers_labels = data2feats(new_tagged_dataset, token_vocab, label_vocab)\n",
    "num_batches_changed_number = int(len(changed_number_feats)/BATCH_SIZE)\n",
    "\n",
    "changed_number_feats_batches = changed_number_feats[:BATCH_SIZE*num_batches_changed_number].view(num_batches_changed_number, BATCH_SIZE, max_len)\n",
    "changed_number_labels_batches = dev_numbers_labels[:BATCH_SIZE*num_batches_changed_number].view(num_batches_changed_number, BATCH_SIZE, max_len)\n",
    "# score = run_eval(changed_number_feats_batches, changed_number_labels_batches, 'BERT')\n",
    "\n",
    "# print('\\033[32mAccuracy for changed number data: \\033[0m {:.4f}'.format(score))\n",
    "\n",
    "# with open(os.path.join('data', 'bert_predictions_numbers.iob2'), 'w') as f:\n",
    "#     for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "#         for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "#             f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# python3 span_f1.py data/bert_predictions_numbers.iob2 data/gold_numbers.iob2 <- run this in terminal to get span f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evuluating LSTM on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dev data: 0.9606\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "def run_eval(feats_batches, labels_batches, model):\n",
    "    if model == 'LSTM':\n",
    "        langid_model.eval()\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for sents, labels in zip(feats_batches, labels_batches):\n",
    "        if model == 'LSTM':\n",
    "            output_scores = langid_model.forward(sents)\n",
    "            predicted_tags  = torch.argmax(output_scores, 2)\n",
    "        elif model == 'BERT':\n",
    "            output_scores = fine_tuned(sents) \n",
    "            predicted_tags  = torch.argmax(output_scores.logits, dim=-1)\n",
    "        else:\n",
    "            print('Please specify supported model.')\n",
    "            return\n",
    "        for sentence in sents:\n",
    "            sentenceWords = []\n",
    "            for wordIndex in sentence:\n",
    "                sentenceWords.append(token_vocab.getWord(wordIndex.item()))\n",
    "            sentences.append(sentenceWords)\n",
    "        for sentenceTags in predicted_tags:\n",
    "                predictionTagOneSentence = []\n",
    "                for tag in sentenceTags:\n",
    "                    predictionTagOneSentence.append(label_vocab.idx2word[tag.item()])\n",
    "                predictions.append(predictionTagOneSentence)\n",
    "        for goldSent, predSent in zip(labels, predicted_tags):\n",
    "            for goldLabel, predLabel in zip(goldSent, predSent):\n",
    "                if goldLabel.item() != 0:\n",
    "                    total += 1\n",
    "                    if goldLabel.item() == predLabel.item():\n",
    "                        match+= 1\n",
    "    return(match/total)\n",
    "\n",
    "\n",
    "num_batches_dev = int(len(dev_feats)/BATCH_SIZE)\n",
    "\n",
    "dev_feats_batches = dev_feats[:BATCH_SIZE*num_batches_dev].view(num_batches_dev, BATCH_SIZE, max_len)\n",
    "dev_labels_batches = dev_labels[:BATCH_SIZE*num_batches_dev].view(num_batches_dev, BATCH_SIZE, max_len)\n",
    "score = run_eval(dev_feats_batches, dev_labels_batches, 'LSTM')\n",
    "\n",
    "print('Accuracy for dev data: {:.4f}'.format(score))\n",
    "\n",
    "with open(os.path.join('data', 'new_lstm_predictions_dev.iob2'), 'w') as f:\n",
    "    for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "        for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# python3 span_f1.py data/new_lstm_predictions_dev.iob2 data/en_ewt-ud-dev.iob2  <- run this in terminal to get span f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n",
      "2001\n",
      "2001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(sentences[20])\n",
    "# print(predictions[20])\n",
    "# print(new_tagged_dataset[20][1])\n",
    "# def toSpans(tags):\n",
    "#     spans = set()\n",
    "#     for beg in range(len(tags)):\n",
    "#         if tags[beg][0] == 'B':\n",
    "#             end = beg\n",
    "#             for end in range(beg+1, len(tags)):\n",
    "#                 if tags[end][0] != 'I':\n",
    "#                     break\n",
    "#             spans.add(str(beg) + '-' + str(end) + ':' + tags[beg][2:])\n",
    "#     return spans\n",
    "\n",
    "# predSpans = toSpans(predictions)\n",
    "# goldSpans = toSpans(new_tagged_dataset)\n",
    "# print(len(predSpans.intersection(goldSpans)))\n",
    "# print(predictions)\n",
    "print(len(predictions))\n",
    "print(len(dev_feats))\n",
    "goldLabels = []\n",
    "for sentence in dev_data:\n",
    "    goldLabels.append(sentence[1])\n",
    "\n",
    "print(len(goldLabels))\n",
    "\n",
    "# sklearn.metrics.confusion_matrix(new_tagged_dataset, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change names for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAccuracy for changed names data: \u001b[0m 0.9126\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "score = run_eval(changed_names_feats_batches, changed_names_labels_batches, 'LSTM')\n",
    "\n",
    "print('\\033[32mAccuracy for changed names data: \\033[0m {:.4f}'.format(score))\n",
    "\n",
    "with open(os.path.join('data', 'new_lstm_predictions_names.iob2'), 'w') as f:\n",
    "    for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "        for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "# python3 span_f1.py data/new_lstm_predictions_names.iob2 data/gold_names.iob2  <- run this in terminal to get span f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change location for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAccuracy for changed location data: \u001b[0m 0.9233\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "score = run_eval(changed_location_feats_batches, changed_location_labels_batches, 'LSTM')\n",
    "\n",
    "print('\\033[32mAccuracy for changed location data: \\033[0m {:.4f}'.format(score))\n",
    "\n",
    "with open(os.path.join('data', 'new_lstm_predictions_location.iob2'), 'w') as f:\n",
    "    for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "        for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# python3 span_f1.py data/new_lstm_predictions_location.iob2 data/gold_location.iob2  <- run this in terminal to get span f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change numbers for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAccuracy for changed number data: \u001b[0m 0.9374\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "score = run_eval(changed_number_feats_batches, changed_number_labels_batches, 'LSTM')\n",
    "\n",
    "print('\\033[32mAccuracy for changed number data: \\033[0m {:.4f}'.format(score))\n",
    "\n",
    "with open(os.path.join('data', 'new_lstm_predictions_numbers.iob2'), 'w') as f:\n",
    "    for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "        for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "            f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# python3 span_f1.py data/new_lstm_predictions_numbers.iob2 data/gold_numbers.iob2  <- run this in terminal to get span f1 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
