{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load untagged data\n",
    "2. Tag them with our LSTM\n",
    "3. Check with original tags and measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"I visited New York last summer.\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"Mount Everest is the world's tallest mountain.\",\n",
    "]\n",
    "\n",
    "# Manually annotated labels for the test data in IOB2 format\n",
    "test_labels = [\n",
    "    [\"O\", \"O\", \"B-LOC\", \"I-LOC\", \"O\", \"O\", \"O\"],\n",
    "    [\"O\", \"B-LOC\", \"I-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    [\"B-LOC\", \"I-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "    # Labels corresponding to each sentence above\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(sentences, vocab, max_len):\n",
    "\n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences]  # Basic tokenization\n",
    "    indexed_sentences = [[vocab.get(token.lower(), vocab['<UNK>']) for token in sentence] for sentence in tokenized_sentences]\n",
    "    padded_sentences = [sentence + [vocab['<PAD>']] * (max_len - len(sentence)) for sentence in indexed_sentences]\n",
    "    return padded_sentences\n",
    "\n",
    "# Assuming `vocab` is a dictionary mapping tokens to indices and `<UNK>` is the unknown token index\n",
    "# Also assuming `max_len` is defined as the maximum sentence length in your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_indices(labels, label_vocab):\n",
    "    return [[label_vocab.get(label, 0) for label in sentence_labels] for sentence_labels in labels]\n",
    "\n",
    "# Assuming `label_vocab` is a dictionary mapping NER labels to indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_data):\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    predictions = []\n",
    "    with torch.no_grad():  # No gradients required for prediction\n",
    "        for sentence in test_data:\n",
    "            sentence_tensor = torch.tensor(sentence).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "            prediction = model(sentence_tensor)\n",
    "            predictions.append(prediction.argmax(dim=2))  # Get the most likely tag index\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, true_labels):\n",
    "    # Flatten lists if they are nested\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    predictions = [p.item() for p in flatten(predictions)]\n",
    "    true_labels = flatten(true_labels)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = sum(1 for pred, true in zip(predictions, true_labels) if pred == true)\n",
    "    total = len(true_labels)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Assuming `predictions` is the output from `predict` function\n",
    "# `true_labels` is the list of label indices corresponding to the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Preprocess the test data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# or whatever max length you have set for your training data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m preprocessed_test_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m test_label_indices \u001b[38;5;241m=\u001b[39m labels_to_indices(test_labels, label_vocab)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load your LSTM model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(sentences, vocab, max_len)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(sentences, vocab, max_len):\n\u001b[0;32m----> 3\u001b[0m     tokenized_sentences \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Basic tokenization\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     indexed_sentences \u001b[38;5;241m=\u001b[39m [[vocab\u001b[38;5;241m.\u001b[39mget(token\u001b[38;5;241m.\u001b[39mlower(), vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sentence] \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tokenized_sentences]\n\u001b[1;32m      5\u001b[0m     padded_sentences \u001b[38;5;241m=\u001b[39m [sentence \u001b[38;5;241m+\u001b[39m [vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m*\u001b[39m (max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence)) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m indexed_sentences]\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(sentences, vocab, max_len):\n\u001b[0;32m----> 3\u001b[0m     tokenized_sentences \u001b[38;5;241m=\u001b[39m [\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences]  \u001b[38;5;66;03m# Basic tokenization\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     indexed_sentences \u001b[38;5;241m=\u001b[39m [[vocab\u001b[38;5;241m.\u001b[39mget(token\u001b[38;5;241m.\u001b[39mlower(), vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sentence] \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tokenized_sentences]\n\u001b[1;32m      5\u001b[0m     padded_sentences \u001b[38;5;241m=\u001b[39m [sentence \u001b[38;5;241m+\u001b[39m [vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m*\u001b[39m (max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence)) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m indexed_sentences]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Load the vocab and label vocab here\n",
    "vocab = {...}\n",
    "label_vocab = {...}\n",
    "\n",
    "# Preprocess the test data\n",
    "max_len = 50  # or whatever max length you have set for your training data\n",
    "preprocessed_test_data = preprocess_data(test_sentences, vocab, max_len)\n",
    "test_label_indices = labels_to_indices(test_labels, label_vocab)\n",
    "\n",
    "\n",
    "model = ...  #trained LSTM model\n",
    "\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
