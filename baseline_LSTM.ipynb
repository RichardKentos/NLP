{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      7\u001b[0m PAD \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m8446\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "PAD = '<PAD>'\n",
    "torch.manual_seed(8446)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iob2_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data\n",
    "\n",
    "train_data= read_iob2_file('./en_ewt-ud-train.iob2')\n",
    "dev_data = read_iob2_file('./en_ewt-ud-dev.iob2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, pad_unk):\n",
    "        \"\"\"\n",
    "        A convenience class that can help store a vocabulary\n",
    "        and retrieve indices for inputs.\n",
    "        \"\"\"\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {self.pad_unk: 0}\n",
    "        self.idx2word = [self.pad_unk]\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                self.word2idx[word] = len(self.idx2word)\n",
    "                self.idx2word.append(word)\n",
    "            else:\n",
    "                return self.word2idx[self.pad_unk]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "\n",
    "max_len = max([len(x[0]) for x in train_data ])\n",
    "\n",
    "# Create vocabularies for both the tokens\n",
    "# and the tags\n",
    "token_vocab = Vocab(PAD)\n",
    "label_vocab = Vocab(PAD)\n",
    "id_to_token = [PAD]\n",
    "\n",
    "for tokens, tags in train_data:\n",
    "    for token in tokens:\n",
    "        token_vocab.getIdx(token, True)\n",
    "    for tag in tags:\n",
    "        label_vocab.getIdx(tag, True)\n",
    "\n",
    "NWORDS = len(token_vocab.idx2word)\n",
    "NTAGS = len(label_vocab.idx2word)\n",
    "\n",
    "# convert text data with labels to indices\n",
    "def data2feats(inputData, word_vocab, label_vocab):\n",
    "    feats = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    labels = torch.zeros((len(inputData), max_len), dtype=torch.long)\n",
    "    for sentPos, sent in enumerate(inputData):\n",
    "        for wordPos, word in enumerate(sent[0][:max_len]):\n",
    "            wordIdx = token_vocab.getIdx(word)\n",
    "            feats[sentPos][wordPos] = wordIdx\n",
    "        for labelPos, label in enumerate(sent[1][:max_len]):\n",
    "            labelIdx = label_vocab.getIdx(label)\n",
    "            labels[sentPos][labelPos] = labelIdx\n",
    "    return feats, labels\n",
    "\n",
    "train_features, train_labels = data2feats(train_data, token_vocab, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to batches\n",
    "num_batches = int(len(train_features)/BATCH_SIZE)\n",
    "train_feats_batches = train_features[:BATCH_SIZE*num_batches].view(num_batches, BATCH_SIZE, max_len)\n",
    "train_labels_batches = train_labels[:BATCH_SIZE*num_batches].view(num_batches, BATCH_SIZE, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model overview: \n",
      "LangID(\n",
      "  (word_embeddings): Embedding(19674, 100)\n",
      "  (bilstm): LSTM(100, 50, batch_first=True)\n",
      "  (hidden_to_tag): Linear(in_features=50, out_features=8, bias=True)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our model consisting of word embeddings, a single bilstm layer, and an output labels\n",
    "class LangID(nn.Module):\n",
    "    def __init__(self, embed_dim, lstm_dim, vocab_dim):\n",
    "        super(LangID, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_dim, embed_dim)\n",
    "        self.bilstm = nn.LSTM(embed_dim, lstm_dim, bidirectional=False, batch_first=True)\n",
    "        self.hidden_to_tag = nn.Linear(lstm_dim, NTAGS)\n",
    "        self.lstm_dim = lstm_dim\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # First encode the input into word representations and run the bilstm\n",
    "        word_vectors = self.word_embeddings(inputs)\n",
    "        bilstm_out, _ = self.bilstm(word_vectors)\n",
    "        #  Now combine (concatenate) the last state of each layer\n",
    "        # backward_out = bilstm_out[:,0,-self.lstm_dim:]\n",
    "        # forward_out = bilstm_out[:,-1,:self.lstm_dim]\n",
    "        # bilstm_out = torch.cat((forward_out, backward_out), 1)\n",
    "        # And get the prediction\n",
    "        y = self.hidden_to_tag(bilstm_out)\n",
    "        return y # softmax this in order to get probs, check out for axis, has to sum up to 1\n",
    "    \n",
    "    # def predict(self, inputs):\n",
    "    #     # Disable updating the weights\n",
    "    #     with torch.no_grad():\n",
    "    #         y = self.forward(inputs)\n",
    "    #         outputs = torch.argmax(y)\n",
    "    #     return outputs\n",
    "\n",
    "\n",
    "# define the model\n",
    "langid_model = LangID(DIM_EMBEDDING, LSTM_HIDDEN, NWORDS)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "optimizer = optim.Adam(langid_model.parameters(), lr=LEARNING_RATE)\n",
    "print('model overview: ')\n",
    "print(langid_model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   loss      Train acc.\n",
      "0       278.73    0.9426\n",
      "1       116.28    0.9668\n",
      "2       60.59     0.9822\n",
      "3       35.76     0.9899\n",
      "4       23.37     0.9935\n"
     ]
    }
   ],
   "source": [
    "print('epoch   loss      Train acc.')\n",
    "for epoch in range(EPOCHS):\n",
    "    langid_model.train() \n",
    "    langid_model.zero_grad()\n",
    "\n",
    "    # Loop over batches\n",
    "    loss = 0\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for batchIdx in range(0, num_batches): # 0, num_batches\n",
    "        output_scores = langid_model.forward(train_feats_batches[batchIdx])\n",
    "        \n",
    "        output_scores = output_scores.view(BATCH_SIZE * max_len, -1)\n",
    "        flat_labels = train_labels_batches[batchIdx].view(BATCH_SIZE * max_len)\n",
    "        batch_loss = loss_function(output_scores, flat_labels)\n",
    "\n",
    "        predicted_labels = torch.argmax(output_scores, 1)\n",
    "        predicted_labels = predicted_labels.view(BATCH_SIZE, max_len)\n",
    "\n",
    "        # Run backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        langid_model.zero_grad()\n",
    "        loss += batch_loss.item()\n",
    "        # Update the number of correct tags and total tags\n",
    "        for gold_sent, pred_sent in zip(train_labels_batches[batchIdx], predicted_labels):\n",
    "            for gold_label, pred_label in zip(gold_sent, pred_sent):\n",
    "                if gold_label != 0:\n",
    "                    total += 1\n",
    "                    if gold_label == pred_label:\n",
    "                        match+= 1\n",
    "    print('{0: <8}{1: <10}{2}'.format(epoch, '{:.2f}'.format(loss/num_batches), '{:.4f}'.format(match / total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'searched', 'all', 'over', 'the', 'internet', ',', 'but', 'I', 'could', 'not', 'find', 'one', 'place', 'in', 'Tampa', 'Bay', 'that', 'sells', 'morcillas', ',', 'also', 'known', 'as', 'blood', 'pudding', ',', 'black', 'pudding', 'and', 'blood', 'sausages', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['I', '<PAD>', 'all', 'over', 'the', 'internet', ',', 'but', 'I', 'could', 'not', 'find', 'one', 'place', 'in', 'Tampa', 'Bay', 'that', '<PAD>', '<PAD>', ',', 'also', 'known', 'as', 'blood', '<PAD>', ',', 'black', '<PAD>', 'and', 'blood', '<PAD>', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Accuracy for dev data: 0.9574\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "def run_eval(feats_batches, labels_batches):\n",
    "    langid_model.eval()\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for sents, labels in zip(feats_batches, labels_batches):\n",
    "        output_scores = langid_model.forward(sents)\n",
    "        predicted_tags  = torch.argmax(output_scores, 2)\n",
    "        for sentence in sents:\n",
    "            sentenceWords = []\n",
    "            for wordIndex in sentence:\n",
    "                sentenceWords.append(token_vocab.getWord(wordIndex.item()))\n",
    "            sentences.append(sentenceWords)\n",
    "        for sentenceTags in predicted_tags:\n",
    "                predictionTagOneSentence = []\n",
    "                for tag in sentenceTags:\n",
    "                    predictionTagOneSentence.append(label_vocab.idx2word[tag.item()])\n",
    "                predictions.append(predictionTagOneSentence)\n",
    "        for goldSent, predSent in zip(labels, predicted_tags):\n",
    "            for goldLabel, predLabel in zip(goldSent, predSent):\n",
    "                if goldLabel.item() != 0:\n",
    "                    total += 1\n",
    "                    if goldLabel.item() == predLabel.item():\n",
    "                        match+= 1\n",
    "    return(match/total)\n",
    "\n",
    "\n",
    "dev_feats, dev_labels = data2feats(dev_data, token_vocab, label_vocab)\n",
    "# print(token_vocab.getWord(dev_feats[0][0].item()))\n",
    "num_batches_dev = int(len(dev_feats)/BATCH_SIZE)\n",
    "\n",
    "dev_feats_batches = dev_feats[:BATCH_SIZE*num_batches_dev].view(num_batches_dev, BATCH_SIZE, max_len)\n",
    "dev_labels_batches = dev_labels[:BATCH_SIZE*num_batches_dev].view(num_batches_dev, BATCH_SIZE, max_len)\n",
    "score = run_eval(dev_feats_batches, dev_labels_batches)\n",
    "\n",
    "print(dev_data[1][0])\n",
    "print(dev_data[1][1])\n",
    "print('')\n",
    "print(sentences[1])\n",
    "print(predictions[1])\n",
    "print('Accuracy for dev data: {:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to iob2 file for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test data: 0.9665\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "predictions = []\n",
    "\n",
    "def run_eval(feats_batches, labels_batches):\n",
    "    langid_model.eval()\n",
    "    match = 0\n",
    "    total = 0\n",
    "    for sents, labels in zip(feats_batches, labels_batches):\n",
    "        output_scores = langid_model.forward(sents)\n",
    "        predicted_tags  = torch.argmax(output_scores, 2)\n",
    "        for sentence in sents:\n",
    "            sentenceWords = []\n",
    "            for wordIndex in sentence:\n",
    "                sentenceWords.append(token_vocab.getWord(wordIndex.item()))\n",
    "            sentences.append(sentenceWords)\n",
    "        for sentenceTags in predicted_tags:\n",
    "                predictionTagOneSentence = []\n",
    "                for tag in sentenceTags:\n",
    "                    predictionTagOneSentence.append(label_vocab.idx2word[tag.item()])\n",
    "                predictions.append(predictionTagOneSentence)\n",
    "        for goldSent, predSent in zip(labels, predicted_tags):\n",
    "            for goldLabel, predLabel in zip(goldSent, predSent):\n",
    "                if goldLabel.item() != 0:\n",
    "                    total += 1\n",
    "                    if goldLabel.item() == predLabel.item():\n",
    "                        match+= 1\n",
    "    return(match/total)\n",
    "\n",
    "test_data = read_iob2_file('./en_ewt-ud-test-masked.iob2')\n",
    "\n",
    "test_feats, test_labels = data2feats(test_data, token_vocab, label_vocab)\n",
    "# print(token_vocab.getWord(dev_feats[0][0].item()))\n",
    "\n",
    "num_batches_test = int(len(test_feats)/BATCH_SIZE)\n",
    "test_feats_batches = test_feats[:BATCH_SIZE*num_batches_test].view(num_batches_test, BATCH_SIZE, max_len)\n",
    "test_labels_batches = test_labels[:BATCH_SIZE*num_batches_test].view(num_batches_test, BATCH_SIZE, max_len)\n",
    "score = run_eval(test_feats_batches, test_labels_batches)\n",
    "\n",
    "\n",
    "print('Accuracy for test data: {:.4f}'.format(score))\n",
    "\n",
    "with open('predictions_test_data.iob2', 'w') as f:\n",
    "        for sent_tokens, sent_preds in zip(sentences, predictions):\n",
    "            for index, (token, pred) in enumerate(zip(sent_tokens, sent_preds)):\n",
    "                f.write(f\"{index}\\t{token}\\t{pred}\\n\")\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['What', 'is', 'this', 'Miramar', '?'], ['O', 'O', 'O', 'O', 'O'])\n",
      "(['What', 'is', 'this', '<PAD>', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictionsTest = read_iob2_file('./predictions_test_data.iob2')\n",
    "\n",
    "print(test_data[0])\n",
    "print(predictionsTest[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
