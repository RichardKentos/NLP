{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - NLP and Deep Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Sequence Prediction with HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement the Viterbi algorithm for decoding in sequence tagging. More concretely, we are going to build a POS tagger for English web data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Emissions and transition probabilities\n",
    "\n",
    "In this part of the exercise you are going to prepare the emission and transition probabilities to use in the viterbi algorithm. We are going to focus on the task of Parts-Of-Speech (POS) tagging. You can use the following datareader for the following assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[0])\n",
    "            current_tags.append(tok[1])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are going to use the the POS labels as indices in our Viterbi matrix, we need to know all labels beforehand, and they need to have a static order. We also need a special beginning and end label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_conll_file('pos-data/en_ewt-train.conll')\n",
    "dev_data = read_conll_file('pos-data/en_ewt-dev.conll')\n",
    "\n",
    "SMOOTH = 0.1\n",
    "BEG = '<S>'\n",
    "END = '</S>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "label_set = set([pos_label for sentence in train_data for pos_label in sentence[1]])\n",
    "label_set.add(BEG)\n",
    "label_set.add(END)\n",
    "# put labels in a list, so that they are guaranteed to have the same order\n",
    "label_list = list(sorted(label_set))\n",
    "\n",
    "print('Length train data: ' + str(len(train_data)))\n",
    "print('Length dev data: ' + str(len(dev_data)))\n",
    "\n",
    "# the data is a list of pairs, containing 1: a list of words 2: a list of POS labels\n",
    "print('Random datapoint:')\n",
    "print(train_data[70][0])\n",
    "print(train_data[70][1])\n",
    "\n",
    "print('All labels:')\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) calculate the transition probabilities based on the training data, use a special label for the beginning of a sentence (`<S>`) and the end of a sentence (`</S>`). use laplace smoothing with a value of 0.1 to avoid probabilities of 0.0.\n",
    "\n",
    "**Hint**: The transition probability $P(t_i|t_{i-1})$ is the probability that given a tag, $t_{i-1}$, that it will be followed by a tag $t_i$. \n",
    "$$P(t_i|t_{i-1}) = {C(t_{i-1},t_{i}) \\over C(t_{i-1})}$$\n",
    "With smoothing:\n",
    "$$P(t_i|t_{i-1}) = {C(t_{i-1},t_{i}) + \\gamma \\over C(t_{i-1}) + (|t|) * \\gamma} $$\n",
    "\n",
    "Where $(|t|-1) * \\gamma$ is used because we want to add probability mass to all labels.\n",
    "\n",
    "**Hint2**: Every sentence in the data looks like `['DET', 'NOUN', 'VERB']` without any `<S>` or `</S>` tags. So the beginning and end of every data sample needs to be handled differently when counting occurences of transitions, alternatively you can add these tokens to each data sample so they look like `['<S>, 'DET', 'NOUN', 'VERB', </S>]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of dictionaries, so that we can easily query for a certain probability.\n",
    "# We add the smoothing value as a starting point, as it has to be added to each count.\n",
    "# Note that a list of lists with POS indices is more efficient (but a bit more cumbersome to implement).\n",
    "transition_counts = {label: {label: SMOOTH for label in label_list} for label in label_list}\n",
    "# The count that a NOUN follows an ADJ (empty now)\n",
    "print(transition_counts['ADJ']['NOUN'])\n",
    "\n",
    "# First obtain the raw counts\n",
    "for sentence in train_data:\n",
    "    labels = [BEG] + sentence[1] + [END]\n",
    "    for pos_idx in range(1,len(labels)):\n",
    "        prev = labels[pos_idx-1]\n",
    "        cur = labels[pos_idx]\n",
    "        transition_counts[prev][cur] += 1\n",
    "        \n",
    "print(transition_counts['ADJ']['NOUN']) # should be 6803.1\n",
    "\n",
    "# Now fill the transition matrix, note that the outgoing probability of each label should sum to 1.0\n",
    "transition_probs = {label: {label: 0.0 for label in label_list} for label in label_list}\n",
    "for prev in label_list:\n",
    "    total_prev = sum(transition_counts[prev].values())\n",
    "    for cur in label_list:\n",
    "        prob = transition_counts[prev][cur]/total_prev\n",
    "        transition_probs[prev][cur] = prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) calculate the emission probabilities based on the training data. Make sure that every POS tag can be assigned to an `<UNK>` token, use laplace smoothing with a value of 0.01 to avoid probabilities of 0.0.\n",
    "\n",
    "**Hint**: The emission probability $P(w_i|t_{i})$ is the probability that given a tag, $t_i$, that it will be associated with a given word $w_i$. The formula below shows counts $C$ needed to calculate the probability.\n",
    "\n",
    "$$P(w_i|t_{i}) = {C(t_{i},w_{i}) \\over C(t_{i})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = {UNK}\n",
    "for sent in train_data:\n",
    "    for word in sent[0]:\n",
    "        word_set.add(word)\n",
    "word_list = list(sorted(word_set))\n",
    "\n",
    "emission_counts = {label: {word: SMOOTH for word in word_list} for label in label_list}\n",
    "\n",
    "for sent in train_data:\n",
    "    for word, label in zip(sent[0], sent[1]):\n",
    "        emission_counts[label][word] += 1\n",
    "\n",
    "\n",
    "emission_probs = {label: {word: SMOOTH for word in word_list} for label in label_list}\n",
    "for label in label_list:\n",
    "    total_label = sum(emission_counts[label].values())\n",
    "    for word in word_list:\n",
    "        emission_probs[label][word] = emission_counts[label][word]/total_label\n",
    "\n",
    "import pickle\n",
    "pickle.dump((transition_probs, emission_probs), open('probs_en.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check whether your solutions are correct by estimating the probabilities on the data and check whether the probabilities match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transition_probs['ADJ']['NOUN']) # 0.5171926196793345\n",
    "print(transition_probs['NOUN']['ADJ']) # 0.01123434129302644\n",
    "print(transition_probs[BEG]['ADJ']) # 0.04082136964025221\n",
    "print(transition_probs['ADJ'][END]) # 0.003808756338424345\n",
    "print(emission_probs['NOUN']['calling'])   # 2.9909103515414666e-05\n",
    "print(emission_probs['VERB']['calling'])  # 0.0005740785225418476\n",
    "print(emission_probs['VERB'][UNK])  # 4.071478883275515e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Viterbi algorithm\n",
    "\n",
    "In the image below we see an example of the calculation of the first 2 positions in a Viterbi decoding:\n",
    "<img width=500px src=\"pics/viterbi.jpg\">\n",
    "\n",
    "* a) Implement Viterbi decoding, use the transition and emission probabilities previously estimated (note that we also provide pre-calculated probabilities in `probs_en.pickle`). You can use the example code shown below as a starting point if you like.\n",
    "\n",
    "**Hint**: The implementation can become simpler if you think about the problem as a 2d matrix that needs to be filled (each position in the list is a node in the viterbi decoding, $v_1(7)$, $v_1(6)$, ...). You can first initialize the matrix with 0.0's, and then fill it from left to right.\n",
    "\n",
    "**Hint2**: You need to combine three probabilities for each possible history, and take the max of all possible histories. You do not need to use negative log probabilities for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also load the pre-calculate probabilities:\n",
    "# import pickle\n",
    "# transition_probs, emission_probs = pickle.load(open('probs_en.pickle', 'rb'))\n",
    "import numpy as np\n",
    "def viterbi(sentence):\n",
    "    \"\"\"\n",
    "    sentence: list of strings\n",
    "    \"\"\"\n",
    "    columns = len(sentence)\n",
    "    # You don't need the special tokens in the viterbi decoding so we remove them\n",
    "    labels_list_exc_special = label_list.copy()\n",
    "    labels_list_exc_special.remove(BEG)\n",
    "    labels_list_exc_special.remove(END)\n",
    "\n",
    "    rows = len(labels_list_exc_special)\n",
    "    \n",
    "    # Create the full matrix for scores as well as the backtracking.\n",
    "    # e.g. scores[0][3] should get the probability of the best path of \n",
    "    # the first label and the 4th word in the sentence\n",
    "    # Backtrack contains the index of the best tag for the previous word\n",
    "    # e.g. backtrack[0][3] should get the index of the best tag for the 3rd word \n",
    "    # when backtracking from the first label and 4th word\n",
    "    scores = np.array([[0.0 for _ in range(columns)] for _ in range(rows)])\n",
    "    backtrack = np.array([[0 for _ in range(columns)] for _ in range(rows)])\n",
    "    \n",
    "    # Handle the first token separately, as it only has 2 probabilities (emission, transition)\n",
    "    word_position = 0\n",
    "    for pos_tag_idx, pos_tag in enumerate(labels_list_exc_special):\n",
    "        # The probability of the first word given the POS tag:\n",
    "        word = sentence[word_position]\n",
    "        if word not in emission_probs[pos_tag]:\n",
    "            word = UNK\n",
    "        em_prob = emission_probs[pos_tag][word] \n",
    "        \n",
    "        # The probability of the POS tag given that the previous \"tag\" is <S>\n",
    "        transition_prob = transition_probs[BEG][pos_tag]\n",
    "        \n",
    "        # Save the total probability:\n",
    "        scores[pos_tag_idx][word_position] = em_prob * transition_prob\n",
    "        \n",
    "        # Backtracking for the first token is uneccessary so we ignore it\n",
    "    \n",
    "    # Now handle the rest of the sequence\n",
    "    for word_position in range(1, columns):\n",
    "        for pos_tag_idx, pos_tag in enumerate(labels_list_exc_special):\n",
    "\n",
    "            # Get emission probability, remember to handle unknown words\n",
    "            word = sentence[word_position]\n",
    "            if word not in emission_probs[pos_tag]:\n",
    "                word = UNK\n",
    "            em_prob = emission_probs[pos_tag][word]\n",
    "            \n",
    "            # For each possible path\n",
    "            # Get the transition probability and the history probability for each candidate tag\n",
    "            # Hint: the history probability is the score of the previous word position in scores matrix\n",
    "            # TODO\n",
    "            candidate_scores = [0]*len(labels_list_exc_special)\n",
    "            for prev_label_idx, prev_label in enumerate(labels_list_exc_special):\n",
    "                history_prob = scores[prev_label_idx][word_position-1]\n",
    "                transition_prob = transition_probs[prev_label][pos_tag]\n",
    "                candidate_scores[prev_label_idx] = history_prob * transition_prob * em_prob\n",
    "                \n",
    "            # Now extract the best score from candidate_scores and its previous path and save these\n",
    "            # Hint: backtrack should contain the index of the best tag backtrack[tag_idx][word_position] = previous_best_tag_idx\n",
    "            # TODO\n",
    "            most_prob = max(candidate_scores)\n",
    "            best_history = candidate_scores.index(most_prob)\n",
    "            backtrack[pos_tag_idx][word_position] = best_history\n",
    "            scores[pos_tag_idx][word_position] = most_prob\n",
    "\n",
    "\n",
    "    # Extract the best score from the last labels to the special end label\n",
    "    # Hint: here you only have history and transition (no emission)\n",
    "    # TODO\n",
    "    candidate_scores = [0.0] * len(labels_list_exc_special)\n",
    "    for prev_label_idx, prev_label in enumerate(labels_list_exc_special):\n",
    "        history_prob = scores[prev_label_idx][-1]\n",
    "        transition_prob = transition_probs[prev_label][END]\n",
    "        score = history_prob * transition_prob\n",
    "        candidate_scores[prev_label_idx] = score\n",
    "    best_score = max(candidate_scores)\n",
    "    best_prev_idx = candidate_scores.index(best_score)\n",
    "    \n",
    "    # Extract the path from the best last label using the backtrack matrix\n",
    "    # Hint: the path contains the index of the best tag for each word\n",
    "    # TODO\n",
    "    cur_path = [best_prev_idx]\n",
    "    for word_idx in reversed(range(len(sentence))):\n",
    "        prev_link = cur_path[-1]\n",
    "        new_link = backtrack[prev_link][word_idx]\n",
    "        cur_path.append(new_link)\n",
    "    \n",
    "    # Reverse the path and convert the indexes to labels\n",
    "    # TODO\n",
    "    return [labels_list_exc_special[ind] for ind in reversed(cur_path)][1:]\n",
    "viterbi(['this', 'is', 'a', 'very', 'good', 'chocolate', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) Ensure that the best path is saved during the decoding, so that you can extract the labels. What is the accuracy of your implementation of the Viterbi algorithm on the development data (`pos-data/en_ewt-dev.conll`)?\n",
    "\n",
    "**Hint**: If implemented correctly, it should score at least an accuracy of 50%. If you score lower, we suggest you try printing the probabilities at each step (word) for the first sentence of the development data.\n",
    "\n",
    "* c) **Bonus**: try to improve your predictions by inspecting common errors, tuning some of the decisions (e.g. smoothing, weighing the three probabilities) you made, or improving the handling of unknown tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = 0.0\n",
    "total = 0\n",
    "for sent in dev_data:\n",
    "    preds = viterbi(sent[0])\n",
    "    for gold, pred in zip(sent[1], preds):\n",
    "        total += 1\n",
    "        if gold == pred:\n",
    "            cor += 1\n",
    "print(cor/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Subword tokenization\n",
    "\n",
    "BERT models are trained to predict tokens that were masked with a special `[mask]` token. In this assignment you will inspect what it has learned, and whether it has certain preferences (i.e. probing). \n",
    "\n",
    "a) Load the multilingual Bert tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokzr = AutoTokenizer.from_pretrained('bert-base-multilingual-cased', use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual BERT was trained on the 100 most frequent languages of Wikipedia. They used smoothing, to correct inbalances in the data. However, their smoothing is relatively conservative, so high-resource languages have a higher impact on the model, and it is unclear how they sampled for training the tokenizer. Compare the tokenizations for two different language types you know; preferably one higher-resource and one lower-resource. If you only know 1 language, or only high-resource languages, try to use a different variety of the language (for example for English, use social media abbreviations or typos, e.g.: c u tmrw). Can you observe any differences in the results? does it match your intuition of separating mostly meaning-carrying subwords?\n",
    "\n",
    "You can use Figure 1 of https://arxiv.org/pdf/1911.02116.pdf or https://en.wikipedia.org/wiki/List_of_Wikipedias to see how large languages are on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'input']\n",
      "['dit', 'is', 'in', 'foar', '##byl', '##d', 'input']\n",
      "['money', 'lover']\n",
      "['geld', 'lief', '##he', '##bber']\n",
      "['Yes', 'exactly', '!', 'Fortuna', '##tely', ',', 'I', 'don', \"'\", 't', 'get', 'them', 'as', 'gift', '##s', 'any', '##more']\n",
      "['ja', ',', 'pr', '##æ', '##cis', '!', 'Held', '##ig', '##vis', 'får', 'jeg', 'dem', 'ikke', 'i', 'gave', 'mere', '.']\n"
     ]
    }
   ],
   "source": [
    "# English vs Frisian\n",
    "print(tokzr.tokenize('this is an example input'))\n",
    "print(tokzr.tokenize('dit is in foarbyld input'))\n",
    "# Exmaple is split into three words in Frisian, but remains a single word in English. \n",
    "# Morphologically, splitting it into two subwords (foar ##byld) could have made sense, \n",
    "# but the language is underrepresented.\n",
    "\n",
    "# English vs Dutch\n",
    "print(tokzr.tokenize('money lover'))\n",
    "print(tokzr.tokenize('geld liefhebber'))\n",
    "# Dutch has a long word, but also 2 times as many subwords, the split of ##he ##bber \n",
    "# would not have occured given a more specialized tokenizer\n",
    "\n",
    "# English vs Danish\n",
    "print(tokzr.tokenize('Yes exactly! Fortunately, I don\\'t get them as gifts anymore'))\n",
    "print(tokzr.tokenize('ja, præcis! Heldigvis får jeg dem ikke i gave mere.'))\n",
    "# Even though they have the same length, the segmentation of præcis is not very \n",
    "# meaningful, of course due to the rare character. On the other hand gift-s vs gave \n",
    "# seems to make sense "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test whether the `bert-base-cased` model can solve the analogy task that we discussed in the word2vec lecture ([slides](https://github.itu.dk/robv/intro-nlp2023/blob/main/slides/07-vector_semantics.pdf), [assignment](https://github.itu.dk/robv/intro-nlp2023/blob/main/assignments/week4/week4.ipynb)), we can do this by masking the target word we are looking for, and let the model predict which words fit best. We can then use a prompt to discover what the language model would guess. For example, we can use the prompt \"man is to king as woman is to [MASK]\". Try at least two syntactic analogies, and two semantic analogies.\n",
    "You can use the following code:\n",
    "\n",
    "(Note that you need 4gb of RAM for this assignment, otherwise you can use the HPC.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['positive', 'simple', 'negative', 'diagnostic', 'successful']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM,AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def getTopN(inputSent, model, tokzr, topn=1):\n",
    "    maskId = tokzr.convert_tokens_to_ids(tokzr.mask_token)\n",
    "    tokenIds = tokzr(inputSent).input_ids\n",
    "    if maskId not in tokenIds:\n",
    "        return 'please include ' + tokzr.mask_token + ' in your input'\n",
    "    maskIndex = tokenIds.index(maskId)\n",
    "    logits = model(torch.tensor([tokenIds])).logits\n",
    "    return tokzr.convert_ids_to_tokens(torch.topk(logits, topn, dim=2).indices[0][maskIndex])\n",
    "\n",
    "tokzr = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-cased')\n",
    "getTopN('This is a [MASK] test.', model, tokzr, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work']\n",
      "['house']\n",
      "['fly']\n",
      "['Norway']\n",
      "['Germany']\n",
      "['Denmark']\n"
     ]
    }
   ],
   "source": [
    "print(getTopN('work is to working as beg is to [MASK].', model, tokzr))\n",
    "print(getTopN('houses is to house as cars is to [MASK].', model, tokzr))\n",
    "\n",
    "print(getTopN('swim is to water as fly is to [MASK].', model, tokzr))\n",
    "print(getTopN('Sjælland is to Denmark as Holland is to [MASK].', model, tokzr))\n",
    "print(getTopN('Copenhagen is to Denmark as Amsterdam is to [MASK].', model, tokzr))\n",
    "print(getTopN('Amsterdam is to The Netherlands as Copenhagen is to [MASK].', model, tokzr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Test how robust the language model is, does it have an effect on the results of the word predictions if you include punctuations at the end of the sentence?, what about starting with a capital? and do typos have a large impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Holland']\n",
      "['be']\n",
      "['Denmark']\n"
     ]
    }
   ],
   "source": [
    "print(getTopN('amsterdam is to the netherlands as copenhagen is to [MASK].', model, tokzr))\n",
    "\n",
    "print(getTopN('Amstrdam is too The Nethrlands as Copnhagen is to [MASK].', model, tokzr))\n",
    "\n",
    "print(getTopN('Amsterdam is to The Netherlands as Københavns is to [MASK].', model, tokzr))\n",
    "\n",
    "# It is pretty sensitive to any alternation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Think of some prompts that test whether the model has any gender biases, you can test this for example by using common gendered names or pronouns, swapping them and then check whether the predicted word changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['farming']\n",
      "['unknown']\n",
      "['unknown']\n",
      "['waiter']\n",
      "['carpenter']\n",
      "['nurse']\n",
      "['His']\n",
      "['Her']\n",
      "['His']\n"
     ]
    }
   ],
   "source": [
    "print(getTopN('Robs occupation is [MASK].', model, tokzr))\n",
    "print(getTopN('Johns occupation is [MASK].', model, tokzr))\n",
    "print(getTopN('Elisa\\'s occupation is [MASK].', model, tokzr))\n",
    "\n",
    "print(getTopN('Rob works as a [MASK].', model, tokzr))\n",
    "print(getTopN('John works as a [MASK].', model, tokzr))\n",
    "print(getTopN('Elisa works as a [MASK].', model, tokzr))\n",
    "\n",
    "print(getTopN('[MASK] occupation is teacher.', model, tokzr))\n",
    "print(getTopN('[MASK] occupation is nurse.', model, tokzr))\n",
    "print(getTopN('[MASK] occupation is researcher.', model, tokzr))\n",
    "\n",
    "# The small sample with names is not consistent enough to draw conclusions from, but the last three show a clear trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Finetune a BERT model\n",
    "\n",
    "We have provided code for training a BERT based classifier, which can be found in `week5/bert/bert-topic.py`. The implementation uses huggingface's transformers library (https://github.com/huggingface/transformers), and simply adds a linear layer to convert the output of the CLS token from the last layer of the masked language model to a label. \n",
    "\n",
    "a) Inspect the code; what should the shape of the output_scores be at the end of the forward pass?, What does this output represent?\n",
    "\n",
    "b) Train the model on your own machine or on the HPC without a GPU (Note that this code needs ~8gb ram), how long does it take?\n",
    "\n",
    "c) Now change the number of maximum training sentences (MAX_TRAIN_SENTS) to 500 and the batch size (BATCH_SIZE) to 32. Note that it will now take very long to train without a GPU. Train the model on the HPC, make sure you reserve a GPU to speed up the training. For more information, see http://hpc.itu.dk/scheduling/templates/gpu/ (only available on ITU network/VPN). Note that the code detects automatically whether a GPU is available. Also note that the transformers library is already installed, and can be loaded with:\n",
    "\n",
    "```\n",
    "module load PyTorch/1.7.1-foss-2020b\n",
    "module load Transformers/4.2.1-foss-2020a-Python-3.8.2\n",
    "``` \n",
    "\n",
    "(which you also have to put in the job script)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Batch size by the number of labels, in our case 8 by 4. This means that there are 4 scores for each instance in a batch, and each score represents the weight the model attributes to a certain class in label2id."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
